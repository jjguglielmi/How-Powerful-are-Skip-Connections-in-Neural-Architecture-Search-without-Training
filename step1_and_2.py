# -*- coding: utf-8 -*-
"""step1_and_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/155he99dWUGPSAfsvAgzg5g-6iCItZGi-

# Import of NATS-Bench and ImageNet16
"""

!pip install nats_bench
!pip install xautodl

!wget 'https://www.dropbox.com/s/pasubh1oghex3g9/?dl=1' -O 'NATS-tss-v1_0-3ffb9-simple.tar'

import tarfile
!wget 'https://www.dropbox.com/s/o2fg17ipz57nru1/?dl=1' -O ImageNet16.tar.gz
file = tarfile.open('ImageNet16.tar.gz')
file.extractall('.')
file.close()
!tar xvf "NATS-tss-v1_0-3ffb9-simple.tar"

"""# Create the API instance for the topology search space in NATS"""

from nats_bench import create
api = create("/content/NATS-tss-v1_0-3ffb9-simple", 'tss', fast_mode=True, verbose=False)

"""# Imports of packages"""

import numpy as np, collections
import pandas as pd
import torch
import torch.nn as nn
import torchvision
import torchvision.datasets as dset
import torchvision.transforms as transforms
from xautodl.models import get_cell_based_tiny_net, get_search_spaces, CellStructure, get_search_spaces
from xautodl.utils import get_model_infos, obtain_accuracy
from xautodl.datasets.DownsampledImageNet import ImageNet16
import random
import os
import copy
import math
from scipy import stats
import time
import collections
import os, sys, time, glob, random, argparse
from copy import deepcopy
from collections import defaultdict

from nats_bench import create
api = create("/content/NATS-tss-v1_0-3ffb9-simple", 'tss', fast_mode=True, verbose=False)
assert torch.cuda.is_available(), "CUDA is not available."
torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark = True

"""# Settings for trainloader and data argumentation"""

def get_datasets(name):
    if name == "cifar10":
        mean = [x / 255 for x in [125.3, 123.0, 113.9]]
        std = [x / 255 for x in [63.0, 62.1, 66.7]]
    elif name == "cifar100":
        mean = [x / 255 for x in [129.3, 124.1, 112.4]]
        std = [x / 255 for x in [68.2, 65.4, 70.4]]
    elif name.startswith("ImageNet16"):
        mean = [x / 255 for x in [122.68, 116.66, 104.01]]
        std = [x / 255 for x in [63.22, 61.26, 65.09]]
    else:
        raise TypeError("Unknown dataset : {:}".format(name))

    # Data Argumentation
    if name == "cifar10" or name == "cifar100":
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])
    elif name.startswith("ImageNet16"):
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])
    
    if name == "cifar10": 
      trainset = dset.CIFAR10("/content/Cifar10", train=True, transform = transform, download=True)
    elif name == "cifar100": 
      trainset = dset.CIFAR100("/content/Cifar100", train=True ,transform = transform, download=True)
    elif name.startswith("ImageNet16"): 
      trainset = ImageNet16("ImageNet16", train=True, transform = transform)
    else:
      raise TypeError("Unknown dataset : {:}".format(name))
    
    batch_size = 32
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=0, pin_memory = True)
    return trainloader

datasets = ["cifar10", "cifar100", "ImageNet16-120"]

"""# Calculation of Hamming Distance as in NASWOT paper -> https://arxiv.org/pdf/2006.04647v3.pdf"""

def hamming_distance(x, y):
  return np.count_nonzero((np.logical_xor(x, y)))

def counting_forward_hook(module, inp, out):
  if isinstance(inp, tuple):
    inp = inp[0]
  inp = inp.view(inp.size(0), -1)
  inp = (inp > 0).float()
  global Ktemp
  res = np.zeros((inp.shape[0],inp.shape[0])) #matrix.shape[0] = vertical, shape[1]=horizontal
  Na = inp.shape[1]
  for i in range(inp.shape[0]):
    res[i,i] = Na
    for j in range(i+1,inp.shape[0]):
      res[i,j] = Na - hamming_distance(inp[i,:].cpu().numpy(), inp[j,:].cpu().numpy()) #dist hamming
      res[j,i] = res[i,j]
  Ktemp = Ktemp + res

"""# Our variation for calculation of Kernel Matrix

"""

def counting_forward_hook(module, inp, out):
  if isinstance(inp, tuple):
      inp = inp[0]
  inp = inp.view(inp.size(0), -1)
  x = (inp > 0).float()
  # K matrix is not calcuated with Hamming Distance 
  K = x @ x.t()
  K2 = (1.-x) @ (1.-x.t())
  global Ktemp
  Ktemp = Ktemp + K.cpu().numpy() + K2.cpu().numpy()


def init(m):
    if isinstance(m, (torch.nn.Conv2d, torch.nn.Linear)):
        torch.nn.init.xavier_normal_(m.weight)

# this is the logarithm of the determinant of K 
def hooklogdet(Ktemp, labels=None):
  s, ld = np.linalg.slogdet(Ktemp)
  return ld

"""# Setting CUDA device and creation of network list"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
result = pd.DataFrame(columns=['Dataset', 'Network', 'Metric', 'Accuracy', 'Time'])

networks = list()

#for each dataset we save all the indexes of the architectures with their own metric scores (Kernel Matrix)
for dataset in datasets:
  trainloader = get_datasets(dataset)
  scores = []
  for i in range(0, 15625):    
    start = time.time()
    config = api.get_net_config(i, dataset) 

    # create the network, which is the sub-class of torch.nn.Module
    network = get_cell_based_tiny_net(config)
    del config
    network.apply(init)

    Ktemp = np.zeros((batch_size, batch_size))
    for name, module in network.named_modules():
      if 'ReLU' in str(type(module)):
        module.register_forward_hook(counting_forward_hook)

    network = network.to(device)
    s = [] 
    data_iterator = iter(trainloader)
    x, target = next(data_iterator)
    x, target = x.to(device), target.to(device)
    network(x)

    s.append(hooklogdet(Ktemp, target)) 
    del Ktemp, network, data_iterator, x, target
    scores[i] = np.mean(s)
    acc = api.get_more_info(i, dataset)["test-accuracy"]
    csv_dict = {'Dataset': dataset , 'Index': i, 'Metric': scores[i], 'Accuracy': acc, 'Time': time.time()-start}

    df_dict = pd.DataFrame([csv_dict])
    result = pd.concat([result, df_dict], ignore_index=True)

    result.to_csv(f'out_{dataset}_last.csv', mode='a', index=False, header=False )
    result = pd.DataFrame(columns=['Dataset', 'Index', 'Metric', 'Accuracy', 'Time'])

"""#NASWOT algorithm"""

bests_nets = []
bests_scores = []
N=20
random.seed(756)
header=['Dataset', 'Index', 'LogDet', 'TestAccuracy', 'Time']
for ds in datasets:
  df= pd.read_csv(f'out_{ds}_last.csv', header=None, names=header)
  for j in range(0,30):
    best_net = None 
    best_score = 0
    for i in range(0,N):
      net = df.loc[df['Index'] == random.randrange(15624)]
      score = net['LogDet'].to_numpy()[0]
      if score > best_score:
          best_score = score
          best_net = net  
      else: continue
    bests_scores.append(best_score)
    bests_nets.append(best_net)

print(bests_scores)

print(bests_nets)
