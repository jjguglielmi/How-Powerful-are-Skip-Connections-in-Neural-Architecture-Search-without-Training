{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project8-Group1_MLDL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YGiPLu1rWX9U",
        "iI9hRkd-Bz4a",
        "gFCZ1Ti7e2n_",
        "2dzDg5duS9Hw",
        "WKOjbEVOtU_3",
        "zJ-BRX9OtG1w",
        "nDUnnc0QSf_9",
        "86w_rZ7hIXle"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nats_bench\n",
        "!pip install xautodl"
      ],
      "metadata": {
        "id": "N1FOaPW8OIWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSSTbRCs52BG"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "#from xautodl.datasets.DownsampledImageNet import ImageNet16\n",
        "from nats_bench import create\n",
        "!wget 'https://www.dropbox.com/s/pasubh1oghex3g9/?dl=1' -O 'NATS-tss-v1_0-3ffb9-simple.tar'\n",
        "#!wget 'https://www.dropbox.com/s/o2fg17ipz57nru1/?dl=1' -O ImageNet16.tar.gz\n",
        "!tar xvf \"NATS-tss-v1_0-3ffb9-simple.tar\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://www.dropbox.com/s/o2fg17ipz57nru1/?dl=1' -O ImageNet16.tar.gz\n",
        "file = tarfile.open('ImageNet16.tar.gz')\n",
        "file.extractall('.')\n",
        "file.close()"
      ],
      "metadata": {
        "id": "0hMby3GbZNmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTS and CREATION of the API instance for the topology search space (TSS) in NATS-Bench"
      ],
      "metadata": {
        "id": "VYXnOJt2Zq11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np, collections\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from xautodl.models import get_cell_based_tiny_net, get_search_spaces, CellStructure, get_search_spaces\n",
        "from xautodl.utils import get_model_infos, obtain_accuracy\n",
        "from xautodl.datasets.DownsampledImageNet import ImageNet16\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "from scipy import stats\n",
        "import time\n",
        "import collections\n",
        "import os, sys, time, glob, random, argparse\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "\n",
        "from nats_bench import create, genotype_utils\n",
        "api = create(\"/content/NATS-tss-v1_0-3ffb9-simple\", 'tss', fast_mode=True, verbose=False)\n",
        "assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "batch_size = 128\n"
      ],
      "metadata": {
        "id": "eN4x_stb6Lsj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings for trainloader and data argumentation "
      ],
      "metadata": {
        "id": "YGiPLu1rWX9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## https://github.com/D-X-Y/AutoDL-Projects/blob/f46486e21b71ae6459a700be720d7648b5429569/xautodl/datasets/get_dataset_with_transform.py#L99\n",
        "def get_datasets(name):\n",
        "    if name == \"cifar10\":\n",
        "        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
        "        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
        "    elif name == \"cifar100\":\n",
        "        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n",
        "        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n",
        "    elif name.startswith(\"ImageNet16\"):\n",
        "        mean = [x / 255 for x in [122.68, 116.66, 104.01]]\n",
        "        std = [x / 255 for x in [63.22, 61.26, 65.09]]\n",
        "    else:\n",
        "        raise TypeError(\"Unknown dataset : {:}\".format(name))\n",
        "\n",
        "    # Data Argumentation\n",
        "    if name == \"cifar10\" or name == \"cifar100\":\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ])\n",
        "    elif name.startswith(\"ImageNet16\"):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ])\n",
        "    \n",
        "    if name == \"cifar10\": \n",
        "      trainset = dset.CIFAR10(\"/content/Cifar10\", train=True, transform = transform, download=True)\n",
        "    elif name == \"cifar100\": \n",
        "      trainset = dset.CIFAR100(\"/content/Cifar100\", train=True ,transform = transform, download=True)\n",
        "    elif name.startswith(\"ImageNet16\"): \n",
        "      trainset = ImageNet16(\"ImageNet16\", train=True, transform = transform)\n",
        "    else:\n",
        "      raise TypeError(\"Unknown dataset : {:}\".format(name))\n",
        "  \n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=0, pin_memory = True)\n",
        "    return trainloader\n",
        "datasets = [\"cifar10\", \"cifar100\", \"ImageNet16-120\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Metric', 'Accuracy', 'Time'])\n"
      ],
      "metadata": {
        "id": "czEvsnSEQQIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SYNFLOW"
      ],
      "metadata": {
        "id": "WR58b8jo2434"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf /content/zerocostnas"
      ],
      "metadata": {
        "id": "9DyL6FgmZSFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SamsungLabs/zero-cost-nas.git\n",
        "import os\n",
        "os.rename(\"zero-cost-nas\", \"zerocostnas\")\n",
        "from zerocostnas.foresight.models import nasbench2, nasbench2_ops\n",
        "from zerocostnas.foresight.pruners import p_utils\n",
        "from zerocostnas.foresight.pruners.measures import *\n",
        "from zerocostnas.foresight.pruners import predictive\n",
        "###SYNFLOW from github\n",
        "#rename folder zero-cost-nas\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def synflow(arch_str, dataset, train_loader, device, dataload='random', dataload_info=1):\n",
        "  start = time.time()\n",
        "  def get_nclasses(dataset):\n",
        "    return 100 if dataset == 'cifar100' else 10 if dataset == 'cifar10' else 120\n",
        "  # We use the NAS-Bench-201 function because NATS-Bench is an extension of NAS-Bench-201\n",
        "  # To use Zero-Cost-Proxies functions as synflow, we need the entire model of the network;\n",
        "  # that's why we use the class model of NAS-Bench-201\n",
        "  net = nasbench2.get_model_from_arch_str(arch_str.tostr(), get_nclasses(dataset))\n",
        "  measures = predictive.find_measures(net, train_loader, (dataload, dataload_info, get_nclasses(dataset)), device, measure_names=['synflow'])\n",
        "  # We decide to not use only synflow as a result, but normalizing it thanks to the natural logarithm of\n",
        "  # (1 + (measures['synflow'])), base e\n",
        "  return math.log1p(measures['synflow']), time.time() - start \n"
      ],
      "metadata": {
        "id": "IrBGyY9JbtcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3da913-eeee-44b6-b2f1-702ca3caa486"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'zero-cost-nas'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 54 (delta 11), reused 45 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = nasbench2.get_model_from_arch_str('|nor_conv_3x3~0|+|nor_conv_3x3~0|nor_conv_3x3~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_3x3~2|', 10)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "eAfYq6EuBtBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REA with NASWOT without modifications"
      ],
      "metadata": {
        "id": "iI9hRkd-Bz4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def counting_forward_hook(module, inp, out):\n",
        "  if isinstance(inp, tuple):\n",
        "      inp = inp[0]\n",
        "  inp = inp.view(inp.size(0), -1)\n",
        "  x = (inp > 0).float()\n",
        "  # K matrix is not calcuated with Hamming Distance \n",
        "  K = x @ x.t()\n",
        "  K2 = (1.-x) @ (1.-x.t())\n",
        "  global Ktemp\n",
        "  Ktemp = Ktemp + K.cpu().numpy() + K2.cpu().numpy()\n",
        "\n",
        "def init(m):\n",
        "    if isinstance(m, (torch.nn.Conv2d, torch.nn.Linear)):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "# this is the logarithm of the determinant of K \n",
        "def hooklogdet(Ktemp, labels=None):\n",
        "  s, ld = np.linalg.slogdet(Ktemp)\n",
        "  return ld\n",
        "  \n",
        "def kernel(arch, dataset, batch_size, api, train_loader):\n",
        "    start = time.time()\n",
        "    index = api.query_index_by_arch(arch.tostr())\n",
        "    config = api.get_net_config(index, dataset)  \n",
        "    \n",
        "    network = get_cell_based_tiny_net(config)\n",
        "    del config\n",
        "    network.apply(init)\n",
        "\n",
        "    global Ktemp\n",
        "    Ktemp = np.zeros((batch_size,batch_size))\n",
        "    for name, module in network.named_modules():\n",
        "      if 'ReLU' in str(type(module)):\n",
        "        module.register_forward_hook(counting_forward_hook)\n",
        "    \n",
        "    network = network.to(device)\n",
        "    data_iterator = iter(train_loader)\n",
        "    x, target = next(data_iterator)\n",
        "    x, target = x.to(device), target.to(device)\n",
        "    network(x)\n",
        "    logdet= hooklogdet(Ktemp, target)\n",
        "    #print(\"logdet: \", logdet)\n",
        "    return logdet, time.time() - start\n",
        "\n",
        "def random_topology_func(op_names, max_nodes=4):\n",
        "  # Return a random architecture\n",
        "  def random_architecture():\n",
        "      genotypes = []\n",
        "      for i in range(1, max_nodes):\n",
        "        xlist = []\n",
        "        for j in range(i):\n",
        "          node_str = \"{:}<-{:}\".format(i, j)\n",
        "          op_name = random.choice(op_names)\n",
        "          xlist.append((op_name, j))\n",
        "        genotypes.append(tuple(xlist))\n",
        "      \n",
        "      return CellStructure(genotypes)\n",
        "      \n",
        "  return random_architecture\n",
        "\n",
        "def mutate_topology_func(op_names):\n",
        "    \"\"\"Computes the architecture for a child of the given parent architecture.\n",
        "    The parent architecture is cloned and mutated to produce the child architecture. \n",
        "    The child architecture is mutated by randomly switch one operation to another.\n",
        "    \"\"\"\n",
        "    def mutate_architecture(parent_arch):\n",
        "        child_arch = deepcopy(parent_arch)\n",
        "        node_id = random.randint(0, len(child_arch.nodes) - 1)\n",
        "        node_info = list(child_arch.nodes[node_id])\n",
        "        snode_id = random.randint(0, len(node_info) - 1)\n",
        "        xop = random.choice(op_names)\n",
        "        while xop == node_info[snode_id][0]:\n",
        "            xop = random.choice(op_names)\n",
        "        node_info[snode_id] = (xop, node_info[snode_id][1])\n",
        "        child_arch.nodes[node_id] = tuple(node_info)\n",
        "        return child_arch\n",
        "\n",
        "    return mutate_architecture\n",
        "\n",
        "model = { \"arch\": None, \"synflow\": None}\n",
        "child = { \"arch\": None, \"synflow\": None}\n",
        "\n",
        "def regularized_evolution_kernel(\n",
        "    cycles,\n",
        "    population_size,\n",
        "    sample_size,\n",
        "    random_arch,\n",
        "    mutate_arch,\n",
        "    api,\n",
        "    dataset,\n",
        "    train_loader\n",
        "):\n",
        "    \"\"\"Algorithm for regularized evolution (i.e. aging evolution).\n",
        "    Follows \"Algorithm 1\" in Real et al. \"Regularized Evolution for Image\n",
        "    Classifier Architecture Search\".\n",
        "    Args:\n",
        "      cycles: the number of cycles the algorithm should run for.\n",
        "      population_size: the number of individuals to keep in the population.\n",
        "      sample_size: the number of individuals that should participate in each tournament.\n",
        "      time_budget: the upper bound of searching cost\n",
        "    Returns:\n",
        "      history: a list of `Model` instances, representing all the models computed\n",
        "          during the evolution experiment.\n",
        "    \"\"\"\n",
        "    population = []\n",
        "    history, total_time_cost = (\n",
        "        [],\n",
        "        [],\n",
        "    )  # Not used by the algorithm, only used to report results.\n",
        "    current_best_index = []\n",
        "    # Initialize the population with random models.\n",
        "    while len(population) < population_size:\n",
        "        model[\"arch\"] = random_arch()\n",
        "        model[\"metric\"], total_cost = kernel( \n",
        "            model[\"arch\"], dataset, batch_size, api, train_loader #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population.append((model[\"metric\"], model[\"arch\"]))\n",
        "        history.append((model[\"metric\"], model[\"arch\"]))\n",
        "        #print(history)\n",
        "        # print(max(history, key=lambda x: x[0]))\n",
        "        total_time_cost.append(total_cost)\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1])\n",
        "        )\n",
        "    \n",
        "    # Carry out evolution in cycles. Each cycle produces a model and removes another.\n",
        "    while len(history) < cycles:\n",
        "        # Sample randomly chosen models from the current population.\n",
        "        start_time, sample = time.time(), []\n",
        "        while len(sample) < sample_size:\n",
        "            # Inefficient, but written this way for clarity. In the case of neural\n",
        "            # nets, the efficiency of this line is irrelevant because training neural\n",
        "            # nets is the rate-determining step.\n",
        "            candidate = random.choice(list(population))\n",
        "            sample.append(candidate)\n",
        "        # The parent is the best model in the sample.\n",
        "   \n",
        "        parent = max(sample, key=lambda i: i[0])\n",
        "  \n",
        "        # Create the child model and store it.\n",
        "        child[\"arch\"] = mutate_arch(parent[1])\n",
        "\n",
        "        child[\"metric\"], total_cost = kernel( \n",
        "            child[\"arch\"], dataset, batch_size, api, train_loader #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        #print(\"c:\\n\", child['arch'], child['metric'])\n",
        "        # Append the info\n",
        "        population[:-1] = population[1:] ##popleft\n",
        "        population.append((child[\"metric\"], child[\"arch\"]))\n",
        "        #print('pop after:\\n', population)\n",
        "        history.append((child[\"metric\"], child[\"arch\"]))\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1])\n",
        "        )\n",
        "        total_time_cost.append(total_cost)\n",
        "        del sample, candidate, parent\n",
        "\n",
        "    return history, current_best_index, total_time_cost\n",
        "\n",
        "search_space = get_search_spaces(\"tss\", \"nats-bench\") \n",
        "random_arch = random_topology_func(op_names = search_space) \n",
        "mutate_arch = mutate_topology_func(search_space)"
      ],
      "metadata": {
        "id": "ppYCedASBfKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To obtain RESULTS"
      ],
      "metadata": {
        "id": "dueeKqnxUmYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ktemp = []\n",
        "for dataset in datasets:\n",
        "  for run in range(15):\n",
        "    train_loader = get_datasets(dataset)\n",
        "    ## REA che ritorna  history e current_best_index\n",
        "    history, current_best_index, total_times = regularized_evolution_kernel(cycles = 200, ## in history = ((acc, arch), (acc2, arch2)....)\n",
        "                                                                    population_size = 50, \n",
        "                                                                    sample_size = 10, \n",
        "                                                                    random_arch= random_arch, \n",
        "                                                                    mutate_arch=mutate_arch,  \n",
        "                                                                    api=api, \n",
        "                                                                    dataset= dataset,\n",
        "                                                                    train_loader = train_loader,\n",
        "                                                                    )\n",
        "    best_arch = max(history, key=lambda x: x[0])[1]\n",
        "    best_score = max(history, key=lambda x: x[0])[0]\n",
        "    #print(best_arch, \" -> score:\", best_score)\n",
        "\n",
        "    index = api.query_index_by_arch(best_arch.tostr()) \n",
        "\n",
        "    csv_dict = {'Dataset': dataset, 'Network_index': index, 'Metric': best_score, 'Accuracy': api.get_more_info(int(index), dataset, hp=\"200\", is_random=False)[\"test-accuracy\"] , 'Time': sum(total_times)}\n",
        "\n",
        "    df_dict = pd.DataFrame([csv_dict])\n",
        "    result = pd.concat([result, df_dict], ignore_index=True)\n",
        "\n",
        "    result.to_csv(f'outKERNELwithoutMOD15runs{dataset}.csv', mode='a', index=False, header=False )\n",
        "    result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Metric', 'Accuracy', 'Time'])"
      ],
      "metadata": {
        "id": "yz2C4o0YT0Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REA with Synflow without modifications\n"
      ],
      "metadata": {
        "id": "gFCZ1Ti7e2n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_topology_func(op_names, max_nodes=4):\n",
        "  # Return a random architecture\n",
        "  def random_architecture():\n",
        "      genotypes = []\n",
        "      for i in range(1, max_nodes):\n",
        "        xlist = []\n",
        "        for j in range(i):\n",
        "          node_str = \"{:}<-{:}\".format(i, j)\n",
        "          op_name = random.choice(op_names)\n",
        "          xlist.append((op_name, j))\n",
        "        genotypes.append(tuple(xlist))\n",
        "      \n",
        "      return CellStructure(genotypes)\n",
        "      \n",
        "  return random_architecture\n",
        "\n",
        "def mutate_topology_func(op_names):\n",
        "    \"\"\"Computes the architecture for a child of the given parent architecture.\n",
        "    The parent architecture is cloned and mutated to produce the child architecture. \n",
        "    The child architecture is mutated by randomly switch one operation to another.\n",
        "    \"\"\"\n",
        "    def mutate_architecture(parent_arch):\n",
        "        child_arch = deepcopy(parent_arch)\n",
        "        node_id = random.randint(0, len(child_arch.nodes) - 1)\n",
        "        node_info = list(child_arch.nodes[node_id])\n",
        "        snode_id = random.randint(0, len(node_info) - 1)\n",
        "        xop = random.choice(op_names)\n",
        "        while xop == node_info[snode_id][0]:\n",
        "            xop = random.choice(op_names)\n",
        "        node_info[snode_id] = (xop, node_info[snode_id][1])\n",
        "        child_arch.nodes[node_id] = tuple(node_info)\n",
        "        return child_arch\n",
        "\n",
        "    return mutate_architecture\n",
        "model = { \"arch\": None, \"synflow\": None}\n",
        "child = { \"arch\": None, \"synflow\": None}\n",
        "\n",
        "def regularized_evolution_synflow(\n",
        "    cycles,\n",
        "    population_size,\n",
        "    sample_size,\n",
        "    random_arch,\n",
        "    mutate_arch,\n",
        "    api,\n",
        "    dataset,\n",
        "    train_loader,\n",
        "    device\n",
        "):\n",
        "\n",
        "    population = []\n",
        "    hist, total_time_cost = (\n",
        "        [],\n",
        "        [],\n",
        "    )  # Not used by the algorithm, only used to report results.\n",
        "    current_best_index = []\n",
        "    # Initialize the population with random models.\n",
        "    #print('inizio:\\n')\n",
        "    while len(population) < population_size:\n",
        "        model[\"arch\"] = random_arch()\n",
        "        #print(model)\n",
        "        model[\"synflow\"], total_cost = synflow( ###### \n",
        "            model[\"arch\"], dataset, train_loader, device, #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population.append(copy.deepcopy(model))\n",
        "        hist.append((model[\"synflow\"], model[\"arch\"]))\n",
        "        total_time_cost.append(total_cost)\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(hist, key=lambda x: x[0])[1]) ############\n",
        "        )\n",
        "    ## population has the same values repeated in it\n",
        "    #print(population)\n",
        "    #print(total_time_cost)\n",
        "    # Carry out evolution in cycles. Each cycle produces a model and removes another.\n",
        "    while len(hist) < cycles:\n",
        "        # Sample randomly chosen models from the current population.\n",
        "        start_time, sample = time.time(), []\n",
        "        while len(sample) < sample_size:\n",
        "            # Inefficient, but written this way for clarity. In the case of neural\n",
        "            # nets, the efficiency of this line is irrelevant because training neural\n",
        "            # nets is the rate-determining step.\n",
        "            candidate = random.choice(list(population))\n",
        "            #print(candidate)\n",
        "            sample.append(candidate) #sample is a list of dicts\n",
        "\n",
        "        # The parent is the best model in the sample.\n",
        "        #print(sample) ##always the same cause of population\n",
        "        parent = max(sample, key=lambda i: i[\"synflow\"]) ## adjust the sample to take the maximum\n",
        "        #print(type(parent))\n",
        "        # Create the child model and store it.\n",
        "        child[\"arch\"] = mutate_arch(parent[\"arch\"])\n",
        "        child[\"synflow\"], total_cost = synflow( ###### \n",
        "            child[\"arch\"], dataset, train_loader, device, #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population[:-1] = population[1:] ##popleft\n",
        "        population[-1] = child\n",
        "        hist.append((child[\"synflow\"], child[\"arch\"]))\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(hist, key=lambda x: x[0])[1]) ############\n",
        "        )\n",
        "        total_time_cost.append(total_cost)\n",
        "\n",
        "    return hist, current_best_index, total_time_cost\n",
        "\n",
        "search_space = get_search_spaces(\"tss\", \"nats-bench\") \n",
        "random_arch = random_topology_func(op_names = search_space) \n",
        "mutate_arch = mutate_topology_func(search_space)\n"
      ],
      "metadata": {
        "id": "cxn0mV8hOviS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To obtain RESULTS"
      ],
      "metadata": {
        "id": "2dzDg5duS9Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ktemp = []\n",
        "for dataset in datasets:\n",
        "  for run in range(15):\n",
        "    train_loader = get_datasets(dataset)\n",
        "    ## REA che ritorna  history e current_best_index\n",
        "    history, current_best_index, total_times = regularized_evolution_synflow(cycles = 200, ## in history = ((acc, arch), (acc2, arch2)....)\n",
        "                                                                    population_size = 50, \n",
        "                                                                    sample_size = 10, \n",
        "                                                                    random_arch= random_arch, \n",
        "                                                                    mutate_arch=mutate_arch,  \n",
        "                                                                    api=api, \n",
        "                                                                    dataset= dataset,\n",
        "                                                                    train_loader = train_loader,\n",
        "                                                                    device=device)\n",
        "    best_arch = max(history, key=lambda x: x[0])[1]\n",
        "    best_score = max(history, key=lambda x: x[0])[0]\n",
        "    #print(best_arch, \" -> score:\", best_score)\n",
        "\n",
        "    index = api.query_index_by_arch(best_arch.tostr()) \n",
        "\n",
        "    csv_dict = {'Dataset': dataset, 'Network_index': index, 'Metric': best_score, 'Accuracy': api.get_more_info(int(index), dataset, hp=\"200\", is_random=False)[\"test-accuracy\"] , 'Time': sum(total_times)}\n",
        "\n",
        "    df_dict = pd.DataFrame([csv_dict])\n",
        "    result = pd.concat([result, df_dict], ignore_index=True)\n",
        "\n",
        "    result.to_csv(f'outSYNFLOWwithoutMOD15runs{dataset}.csv', mode='a', index=False, header=False )\n",
        "    result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Metric', 'Accuracy', 'Time'])"
      ],
      "metadata": {
        "id": "uCMP70vnS9H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPERATIONS to do on child architecture during \"modified mutation\""
      ],
      "metadata": {
        "id": "WKOjbEVOtU_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mutation_without_skip(child_arch):\n",
        "  names = ['nor_conv_1x1', 'nor_conv_3x3', 'avg_pool_3x3', 'none']\n",
        "  node_id = random.randint(0, len(child_arch.nodes) - 2) #escludo dalla mutation il nodo 2\n",
        "  node_info = list(child_arch.nodes[node_id])\n",
        "  snode_id = random.randint(0, len(node_info) - 1)\n",
        "  xop = random.choice(names)\n",
        "  while xop == node_info[snode_id][0]:\n",
        "      xop = random.choice(names)\n",
        "  \n",
        "  node_info[snode_id] = (xop, node_info[snode_id][1])\n",
        "  child_arch.nodes[node_id] = tuple(node_info)\n",
        "  return child_arch\n",
        "\n",
        "def inversion(child_arch, op_names):\n",
        "  #print(child_arch)\n",
        "  if(child_arch.nodes[2][1][0].find('skip') == 0):\n",
        "    node_info = list(child_arch.nodes[2])\n",
        "    #print(node_info[0])\n",
        "    temp = node_info[0]\n",
        "    node_info[0] = (child_arch.nodes[2][1][0], 0)\n",
        "    node_info[1] = (temp[0], 1)\n",
        "    child_arch.nodes[2]= tuple(node_info)\n",
        "    #child_arch.nodes[2][1] = tuple(temp)\n",
        "    return child_arch\n",
        "  else:\n",
        "    node_info = list(child_arch.nodes[2])\n",
        "    #print(node_info[0])\n",
        "    temp = node_info[0]\n",
        "    node_info[0] = (child_arch.nodes[2][2][0], 0)\n",
        "    node_info[2] = (temp[0], 2)\n",
        "    child_arch.nodes[2]= tuple(node_info)\n",
        "    #child_arch.nodes[2][1] = tuple(temp)\n",
        "    return child_arch\n",
        "\n",
        "def first_node(child_arch):\n",
        "  #skip al node_id0 da spostare alla pos 0 del nodo 2 e inversion\n",
        "    node2_info = list(child_arch.nodes[2])\n",
        "    node0_info = list(child_arch.nodes[0])\n",
        "    temp2 = node2_info[0]\n",
        "    node2_info[0] = (child_arch.nodes[0][0][0], 0)\n",
        "    node0_info[0] = (temp2[0], 0)\n",
        "    child_arch.nodes[2]= tuple(node2_info)\n",
        "    child_arch.nodes[0]= node0_info\n",
        "    return child_arch\n",
        "  \n",
        "def second_node(child_arch):\n",
        "    #skip al node_id1 nodo da spostare alla pos 0 del nodo 2 (prima invertire con pos 1 del node_id1 ) e crossover\n",
        "    if(child_arch.nodes[1][1][0].count('skip') == 1):\n",
        "      info = list(child_arch.nodes[1])\n",
        "      t0 = info[0]\n",
        "      #print(info, t0, child_arch.nodes[1][1][0])\n",
        "      info[0] = (child_arch.nodes[1][1][0], 0)\n",
        "      info[1] = (t0[0], 1)\n",
        "      #print(info)\n",
        "      child_arch.nodes[1]=tuple(info)\n",
        "\n",
        "    node2_info = list(child_arch.nodes[2])\n",
        "    node1_info = list(child_arch.nodes[1])\n",
        "    temp2_0 = node2_info[0]\n",
        "    temp2_1 = node2_info[1]\n",
        "    #print(temp2_0, temp2_1)\n",
        "    node2_info[0] = (child_arch.nodes[1][0][0], 0)\n",
        "    node2_info[1] = (child_arch.nodes[1][1][0], 1)\n",
        "    node1_info[0] = temp2_0\n",
        "    node1_info[1] = temp2_1\n",
        "    #print(node2_info, node1_info)\n",
        "    child_arch.nodes[2]= tuple(node2_info)\n",
        "    child_arch.nodes[1]= tuple(node1_info)\n",
        "    return child_arch"
      ],
      "metadata": {
        "id": "0kzoERfpKT_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REA with NASWOT with condition on SKIP_CONNECTION\n",
        "Considering a minibatch of data $X = \\{  {x_{i} \\} }_{i=1}^{N}$ and a network, we can define an indicator variable $c_{i}$ that form a binary code that corresponds to a linear region. We can use the hamming distance between two data points $d_{H}(c_{i},c_{j})$ to measure how dissimilar the two points are. The hamming distance is the minimum number of changes needed to turn one binary code in the other. Or we could also say it is the number of positions where the two codes differ. The correspondance between binary codes can then be calculated by $K_{H}$ with $N_{A}$ as the number of ReLu activations in the network.\n",
        "\n",
        "$$\n",
        "K_{H} = \\begin{bmatrix} \n",
        "    N_{A} - d_{H}(c_{1},c_{1}) & \\dots & N_{A} - d_{H}(c_{1},c_{N}) \\\\\n",
        "    \\vdots & \\ddots & \\vdots \\\\\n",
        "    N_{A} - d_{H}(c_{N},c_{1}) &  \\dots      & N_{A} - d_{H}(c_{N},c_{N}) \n",
        "    \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The scoring metric $s$ is therefore derived from the logarithm of the Kernel norm at initialization. \n",
        "\n",
        "$$\n",
        "s = \\log \\lVert K_{H} \\rVert\n",
        "$$\n"
      ],
      "metadata": {
        "id": "zJ-BRX9OtG1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_naswotmod_func(op_names, max_nodes=4):\n",
        "  # Return a random architecture\n",
        "  def random_m_architecture():\n",
        "      genotypes = []\n",
        "      found = False\n",
        "      while found == False:\n",
        "        for i in range(1, max_nodes):\n",
        "            xlist = []\n",
        "            for j in range(i):\n",
        "                op_name = random.choice(op_names)\n",
        "                xlist.append((op_name, j))\n",
        "            genotypes.append(tuple(xlist))\n",
        "\n",
        "        if (CellStructure(genotypes).tostr().count('skip')==1): #### controllo skip in \n",
        "          found=True\n",
        "        else:\n",
        "          genotypes = []\n",
        "      \n",
        "      return CellStructure(genotypes)\n",
        "      \n",
        "  return random_m_architecture\n",
        "\n",
        "def mutate_naswotmod_func(op_names):\n",
        "    \"\"\"Computes the architecture for a child of the given parent architecture.\n",
        "    The parent architecture is cloned and mutated to produce the child architecture. \n",
        "    The child architecture is mutated by randomly switch one operation to another.\n",
        "    \"\"\"\n",
        "    def mutate_m_architecture(parent_arch):\n",
        "        child_arch = deepcopy(parent_arch)\n",
        "        ## three ways:\n",
        "        if(child_arch.nodes[2][0][0].find('skip') == 0):## if is in node 2 in pos 0: mutation without add other skip\n",
        "          return mutation_without_skip(child_arch)\n",
        "        elif (child_arch.nodes[2][1][0].find('skip') ==0 or child_arch.nodes[2][2][0].find('skip')== 0):## if is in node 2 but NOT in pos 0: inversion with pos 1\n",
        "          return inversion(child_arch, op_names)\n",
        "        else: ## if skip is NOT in node 2 -> crossover (child_arch is a cellStructure)\n",
        "          arch_to_str = CellStructure.tostr(child_arch).split('+') #list of elements in each nodes\n",
        "          if(arch_to_str[0].count('skip') == 1): #skip nel primo nodo\n",
        "            return first_node(child_arch)\n",
        "          \n",
        "        return second_node(child_arch)\n",
        "\n",
        "    return mutate_m_architecture\n",
        "\n",
        "model = { \"arch\": None, \"metric\": None}\n",
        "child = { \"arch\": None, \"metric\": None}\n",
        "\n",
        "\n",
        "## P/S of: 100/2, 100/50, 20/20, 100/25, 54/16 ; best is 100/25\n",
        "def regularized_evolution_kernel_m(\n",
        "    cycles,\n",
        "    population_size,\n",
        "    sample_size,\n",
        "    random_arch,\n",
        "    mutate_arch,\n",
        "    api,\n",
        "    dataset,\n",
        "    train_loader\n",
        "):\n",
        "\n",
        "    population = []\n",
        "    history, total_time_cost = (\n",
        "        [],\n",
        "        [],\n",
        "    )  # Not used by the algorithm, only used to report results.\n",
        "    current_best_index = []\n",
        "    # Initialize the population with random models.\n",
        "    while len(population) < population_size:\n",
        "        model[\"arch\"] = random_arch()\n",
        "        model[\"metric\"], total_cost = kernel( ###### \n",
        "            model[\"arch\"], dataset, batch_size, api, train_loader #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population.append((model[\"metric\"], model[\"arch\"]))\n",
        "        history.append((model[\"metric\"], model[\"arch\"]))\n",
        "        #print(history)\n",
        "        # print(max(history, key=lambda x: x[0]))\n",
        "        total_time_cost.append(total_cost)\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1])\n",
        "        )\n",
        "    #print(total_time_cost)\n",
        "    #print(\"pop: \\n\", population)\n",
        "    \n",
        "    # Carry out evolution in cycles. Each cycle produces a model and removes another.\n",
        "    while len(history) < cycles:\n",
        "        # Sample randomly chosen models from the current population.\n",
        "        start_time, sample = time.time(), []\n",
        "        while len(sample) < sample_size:\n",
        "            # Inefficient, but written this way for clarity. In the case of neural\n",
        "            # nets, the efficiency of this line is irrelevant because training neural\n",
        "            # nets is the rate-determining step.\n",
        "            candidate = random.choice(list(population))\n",
        "            sample.append(candidate)\n",
        "        # The parent is the best model in the sample.\n",
        "        #print(\"sample:\\n\", sample)\n",
        "        parent = max(sample, key=lambda i: i[0])\n",
        "        #print(\"p:\\n\", parent[1], parent[0])\n",
        "        # Create the child model and store it.\n",
        "        child[\"arch\"] = mutate_arch(parent[1])\n",
        "        #print(\"c:\\n\", child['arch'])\n",
        "        child[\"metric\"], total_cost = kernel( ###### \n",
        "            child[\"arch\"], dataset, batch_size, api, train_loader #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        #print(\"c:\\n\", child['arch'], child['metric'])\n",
        "        # Append the info\n",
        "        population[:-1] = population[1:] ##popleft\n",
        "        population.append((child[\"metric\"], child[\"arch\"]))\n",
        "        #print('pop after:\\n', population)\n",
        "        history.append((child[\"metric\"], child[\"arch\"]))\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1])\n",
        "        )\n",
        "        total_time_cost.append(total_cost)\n",
        "        del sample, candidate, parent\n",
        "\n",
        "    return history, current_best_index, total_time_cost\n",
        "\n",
        "search_space = get_search_spaces(\"tss\", \"nats-bench\") \n",
        "random_naswotmod = random_naswotmod_func(op_names = search_space) \n",
        "mutate_naswotmod = mutate_naswotmod_func(search_space)"
      ],
      "metadata": {
        "id": "YE1VmNBcDypy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To obtain RESULTS"
      ],
      "metadata": {
        "id": "nDUnnc0QSf_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ktemp = []\n",
        "for dataset in datasets:\n",
        "  for run in range(15):\n",
        "    train_loader = get_datasets(dataset)\n",
        "    ## REA che ritorna  history e current_best_index\n",
        "    history, current_best_index, total_times = regularized_evolution_kernel_m(cycles = 200, ## in history = ((acc, arch), (acc2, arch2)....)\n",
        "                                                                    population_size = 50, \n",
        "                                                                    sample_size = 10, \n",
        "                                                                    random_arch= random_naswotmod, \n",
        "                                                                    mutate_arch=mutate_naswotmod,  \n",
        "                                                                    api=api, \n",
        "                                                                    dataset= dataset,\n",
        "                                                                    train_loader = train_loader,\n",
        "                                                                    )\n",
        "    best_arch = max(history, key=lambda x: x[0])[1]\n",
        "    best_score = max(history, key=lambda x: x[0])[0]\n",
        "    #print(best_arch, \" -> score:\", best_score)\n",
        "\n",
        "    index = api.query_index_by_arch(best_arch.tostr()) \n",
        "\n",
        "    csv_dict = {'Dataset': dataset, 'Network_index': index, 'Metric': best_score, 'Accuracy': api.get_more_info(int(index), dataset, hp=\"200\", is_random=False)[\"test-accuracy\"] , 'Time': sum(total_times)}\n",
        "\n",
        "    df_dict = pd.DataFrame([csv_dict])\n",
        "    result = pd.concat([result, df_dict], ignore_index=True)\n",
        "\n",
        "    result.to_csv(f'outKERNELwithoutMOD15runs{dataset}.csv', mode='a', index=False, header=False )\n",
        "    result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Metric', 'Accuracy', 'Time'])"
      ],
      "metadata": {
        "id": "oXJeTp8TSf_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REA with Synflow with condition on SKIP_CONNECTION"
      ],
      "metadata": {
        "id": "86w_rZ7hIXle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_synflowmod_func(op_names, max_nodes=4):\n",
        "  # Return a random architecture\n",
        "  def random_m_architecture():\n",
        "      genotypes = []\n",
        "      found = False\n",
        "      while found == False:\n",
        "        for i in range(1, max_nodes):\n",
        "            xlist = []\n",
        "            for j in range(i):\n",
        "                op_name = random.choice(op_names)\n",
        "                xlist.append((op_name, j))\n",
        "            genotypes.append(tuple(xlist))\n",
        "\n",
        "        if (CellStructure(genotypes).tostr().count('skip')==1): #### controllo skip in \n",
        "          found=True\n",
        "        else:\n",
        "          genotypes = []\n",
        "      \n",
        "      return CellStructure(genotypes)\n",
        "      \n",
        "  return random_m_architecture\n",
        "\n",
        "def mutate_synflowmod_func(op_names):\n",
        "    \"\"\"Computes the architecture for a child of the given parent architecture.\n",
        "    The parent architecture is cloned and mutated to produce the child architecture. \n",
        "    The child architecture is mutated by randomly switch one operation to another.\n",
        "    \"\"\"\n",
        "    def mutate_m_architecture(parent_arch):\n",
        "        child_arch = deepcopy(parent_arch)\n",
        "        ## three ways:\n",
        "        if(child_arch.nodes[2][0][0].find('skip') == 0):## if is in node 2 in pos 0: mutation without add other skip\n",
        "          return mutation_without_skip(child_arch)\n",
        "        elif (child_arch.nodes[2][1][0].find('skip') ==0 or child_arch.nodes[2][2][0].find('skip')== 0):## if is in node 2 but NOT in pos 0: inversion with pos 1\n",
        "          return inversion(child_arch, op_names)\n",
        "        else: ## if skip is NOT in node 2 -> crossover (child_arch is a cellStructure)\n",
        "          arch_to_str = CellStructure.tostr(child_arch).split('+') #list of elements in each nodes\n",
        "          if(arch_to_str[0].count('skip') == 1): #skip nel primo nodo\n",
        "            return first_node(child_arch)\n",
        "          \n",
        "        return second_node(child_arch)\n",
        "\n",
        "    return mutate_m_architecture\n",
        "\n",
        "model = { \"arch\": None, \"synflow\": None}\n",
        "child = { \"arch\": None, \"synflow\": None}\n",
        "\n",
        "\n",
        "## P/S of: 100/2, 100/50, 20/20, 100/25, 54/16 ; best is 100/25\n",
        "def regularized_evolution_synflow_m(\n",
        "    cycles,\n",
        "    population_size,\n",
        "    sample_size,\n",
        "    random_arch,\n",
        "    mutate_arch,\n",
        "    api,\n",
        "    dataset,\n",
        "    train_loader,\n",
        "    device\n",
        "):\n",
        "\n",
        "    population = []\n",
        "    history, total_time_cost = (\n",
        "        [],\n",
        "        [],\n",
        "    )  # Not used by the algorithm, only used to report results.\n",
        "    current_best_index = []\n",
        "    # Initialize the population with random models.\n",
        "    while len(population) < population_size:\n",
        "        model[\"arch\"] = random_arch()\n",
        "        model[\"synflow\"], total_cost = synflow( ###### \n",
        "            model[\"arch\"], dataset, train_loader, device, #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population.append(copy.deepcopy(model))\n",
        "        history.append((model[\"synflow\"], model[\"arch\"]))\n",
        "        total_time_cost.append(total_cost)\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1]) ############\n",
        "        )\n",
        "    ## population has the same values repeated in it\n",
        "    #print(population)\n",
        "    #print(total_time_cost)\n",
        "    # Carry out evolution in cycles. Each cycle produces a model and removes another.\n",
        "    while len(history) < cycles:\n",
        "        # Sample randomly chosen models from the current population.\n",
        "        start_time, sample = time.time(), []\n",
        "        while len(sample) < sample_size:\n",
        "            # Inefficient, but written this way for clarity. In the case of neural\n",
        "            # nets, the efficiency of this line is irrelevant because training neural\n",
        "            # nets is the rate-determining step.\n",
        "            candidate = random.choice(list(population))\n",
        "            #print(candidate)\n",
        "            sample.append(candidate) #sample is a list of dicts\n",
        "\n",
        "        # The parent is the best model in the sample.\n",
        "        #print(sample) ##always the same cause of population\n",
        "        parent = max(sample, key=lambda i: i[\"synflow\"]) ## adjust the sample to take the maximum\n",
        "        #print(type(parent))\n",
        "        # Create the child model and store it.\n",
        "        child[\"arch\"] = mutate_arch(parent[\"arch\"])\n",
        "        child[\"synflow\"], total_cost = synflow( ###### \n",
        "            child[\"arch\"], dataset, train_loader, device, #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population[:-1] = population[1:] ##popleft\n",
        "        population[-1] = child\n",
        "        history.append((child[\"synflow\"], child[\"arch\"]))\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1]) ############\n",
        "        )\n",
        "        total_time_cost.append(total_cost)\n",
        "        del sample, candidate, parent\n",
        "\n",
        "    return history, current_best_index, total_time_cost\n",
        "\n",
        "search_space = get_search_spaces(\"tss\", \"nats-bench\") \n",
        "random_synflowmod = random_synflowmod_func(op_names = search_space) \n",
        "mutate_synflowmod = mutate_synflowmod_func(search_space)"
      ],
      "metadata": {
        "id": "OgPFSdACIU2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To obtain RESULTS"
      ],
      "metadata": {
        "id": "5HaBFXGqUKwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ktemp = []\n",
        "for dataset in datasets:\n",
        "  for run in range(15):\n",
        "    train_loader = get_datasets(dataset)\n",
        "    ## REA che ritorna  history e current_best_index\n",
        "    history, current_best_index, total_times = regularized_evolution_synflow_m(cycles = 200, ## in history = ((acc, arch), (acc2, arch2)....)\n",
        "                                                                    population_size = 50, \n",
        "                                                                    sample_size = 10, \n",
        "                                                                    random_arch= random_synflowmod, \n",
        "                                                                    mutate_arch=mutate_synflowmod,  \n",
        "                                                                    api=api, \n",
        "                                                                    dataset= dataset,\n",
        "                                                                    train_loader = train_loader,\n",
        "                                                                    device=device)\n",
        "    best_arch = max(history, key=lambda x: x[0])[1]\n",
        "    best_score = max(history, key=lambda x: x[0])[0]\n",
        "    #print(best_arch, \" -> score:\", best_score)\n",
        "\n",
        "    index = api.query_index_by_arch(best_arch.tostr()) \n",
        "\n",
        "    csv_dict = {'Dataset': dataset, 'Network_index': index, 'Metric': best_score, 'Accuracy': api.get_more_info(int(index), dataset, hp=\"200\", is_random=False)[\"test-accuracy\"] , 'Time': sum(total_times)}\n",
        "\n",
        "    df_dict = pd.DataFrame([csv_dict])\n",
        "    result = pd.concat([result, df_dict], ignore_index=True)\n",
        "\n",
        "    result.to_csv(f'outSYNFLOWwithMOD15runs{dataset}.csv', mode='a', index=False, header=False )\n",
        "    result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Metric', 'Accuracy', 'Time'])"
      ],
      "metadata": {
        "id": "Yr99bNbiUKFV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}