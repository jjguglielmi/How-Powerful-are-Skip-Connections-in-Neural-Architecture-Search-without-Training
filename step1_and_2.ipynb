{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import of NATS-Bench and ImageNet16"
      ],
      "metadata": {
        "id": "5gvVsm8PYotz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XPvwQxbrEkG"
      },
      "outputs": [],
      "source": [
        "!pip install nats_bench\n",
        "!pip install xautodl\n",
        "\n",
        "!wget 'https://www.dropbox.com/s/pasubh1oghex3g9/?dl=1' -O 'NATS-tss-v1_0-3ffb9-simple.tar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NDQWCKsMkCQ"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "!wget 'https://www.dropbox.com/s/o2fg17ipz57nru1/?dl=1' -O ImageNet16.tar.gz\n",
        "file = tarfile.open('ImageNet16.tar.gz')\n",
        "file.extractall('.')\n",
        "file.close()\n",
        "!tar xvf \"NATS-tss-v1_0-3ffb9-simple.tar\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the API instance for the topology search space in NATS"
      ],
      "metadata": {
        "id": "10U6YeD85RrD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWO7wswHqkpd"
      },
      "outputs": [],
      "source": [
        "from nats_bench import create\n",
        "api = create(\"/content/NATS-tss-v1_0-3ffb9-simple\", 'tss', fast_mode=True, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports of packages"
      ],
      "metadata": {
        "id": "NVExtE_n5XnW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3aPDfGOriYT"
      },
      "outputs": [],
      "source": [
        "import numpy as np, collections\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from xautodl.models import get_cell_based_tiny_net, get_search_spaces, CellStructure, get_search_spaces\n",
        "from xautodl.utils import get_model_infos, obtain_accuracy\n",
        "from xautodl.datasets.DownsampledImageNet import ImageNet16\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "from scipy import stats\n",
        "import time\n",
        "import collections\n",
        "import os, sys, time, glob, random, argparse\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings for trainloader and data augmentation"
      ],
      "metadata": {
        "id": "krpee2MD5kQD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkLtsVAwR2Kr"
      },
      "outputs": [],
      "source": [
        "def get_datasets(name):\n",
        "    if name == \"cifar10\":\n",
        "        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
        "        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
        "    elif name == \"cifar100\":\n",
        "        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n",
        "        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n",
        "    elif name.startswith(\"ImageNet16\"):\n",
        "        mean = [x / 255 for x in [122.68, 116.66, 104.01]]\n",
        "        std = [x / 255 for x in [63.22, 61.26, 65.09]]\n",
        "    else:\n",
        "        raise TypeError(\"Unknown dataset : {:}\".format(name))\n",
        "\n",
        "    # Data Argumentation\n",
        "    if name == \"cifar10\" or name == \"cifar100\":\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ])\n",
        "    elif name.startswith(\"ImageNet16\"):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ])\n",
        "    \n",
        "    if name == \"cifar10\": \n",
        "      trainset = dset.CIFAR10(\"/content/Cifar10\", train=True, transform = transform, download=True)\n",
        "    elif name == \"cifar100\": \n",
        "      trainset = dset.CIFAR100(\"/content/Cifar100\", train=True ,transform = transform, download=True)\n",
        "    elif name.startswith(\"ImageNet16\"): \n",
        "      trainset = ImageNet16(\"ImageNet16\", train=True, transform = transform)\n",
        "    else:\n",
        "      raise TypeError(\"Unknown dataset : {:}\".format(name))\n",
        "    \n",
        "    batch_size = 128\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=0, pin_memory = True)\n",
        "    return trainloader, batch_size\n",
        "\n",
        "datasets = [\"cifar10\", \"cifar100\", \"ImageNet16-120\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculation of Hamming Distance as in NASWOT paper -> https://arxiv.org/pdf/2006.04647v3.pdf"
      ],
      "metadata": {
        "id": "W6hmAOmD5ooC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hamming_distance(x, y):\n",
        "  return np.count_nonzero((np.logical_xor(x, y)))\n",
        "\n",
        "def counting_forward_hook(module, inp, out):\n",
        "  if isinstance(inp, tuple):\n",
        "    inp = inp[0]\n",
        "  inp = inp.view(inp.size(0), -1)\n",
        "  inp = (inp > 0).float()\n",
        "  global Ktemp\n",
        "  res = np.zeros((inp.shape[0],inp.shape[0])) \n",
        "  Na = inp.shape[1]\n",
        "  for i in range(inp.shape[0]):\n",
        "    res[i,i] = Na #on the diagonal there are elements with high similarity\n",
        "    for j in range(i+1,inp.shape[0]):\n",
        "      res[i,j] = Na - hamming_distance(inp[i,:].cpu().numpy(), inp[j,:].cpu().numpy())  #hamming distance\n",
        "      res[j,i] = res[i,j]\n",
        "  Ktemp = Ktemp + res\n"
      ],
      "metadata": {
        "id": "rs0JvbXVObRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variation for calculation of Kernel Matrix\n"
      ],
      "metadata": {
        "id": "w_hpXXXQ55ib"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rlqttmEWPF0"
      },
      "outputs": [],
      "source": [
        "def counting_forward_hook(module, inp, out):\n",
        "  if isinstance(inp, tuple):\n",
        "      inp = inp[0]\n",
        "  inp = inp.view(inp.size(0), -1)\n",
        "  x = (inp > 0).float()\n",
        "  # K matrix is not calcuated with Hamming Distance but with Simon H Operator \n",
        "  K = x @ x.t() #dot product between tensor (all ones) and its own transposed\n",
        "  K2 = (1.-x) @ (1.-x.t()) #dot product between tensor (all zeros) and its transposed\n",
        "  global Ktemp\n",
        "  Ktemp = Ktemp + K.cpu().numpy() + K2.cpu().numpy()\n",
        "\n",
        "\n",
        "def init(m):\n",
        "    if isinstance(m, (torch.nn.Conv2d, torch.nn.Linear)):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "# this is the logarithm of the determinant of K \n",
        "def hooklogdet(Ktemp, labels=None):\n",
        "  s, ld = np.linalg.slogdet(Ktemp)\n",
        "  return ld"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting CUDA device\n"
      ],
      "metadata": {
        "id": "smnqUkhb6DfC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ5T2evFp8lt"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 1 - Score at initialization"
      ],
      "metadata": {
        "id": "P-shEpYYq5TS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ECDrm5OLnBL"
      },
      "outputs": [],
      "source": [
        "for dataset in datasets:\n",
        "  train_loader, batch_size = get_datasets(dataset)\n",
        "  #for each dataset we save all the indexes of the architectures with:\n",
        "  # -their own metric scores (Kernel Matrix)\n",
        "  # -the correspondent accuracy score for 200 epochs\n",
        "  # -time to configurate the network, calculate the logarithm of the determinant and output the accuracy\n",
        "  for i in range(len(api)):    \n",
        "    start = time.time()\n",
        "    config = api.get_net_config(i, dataset)\n",
        "    network = get_cell_based_tiny_net(config)\n",
        "    network.apply(init)\n",
        "\n",
        "    for name, module in network.named_modules():\n",
        "      if (isinstance(module, torch.nn.modules.activation.ReLU)):\n",
        "        # register on each Relu a forward hook\n",
        "        module.register_forward_hook(counting_forward_hook)\n",
        "\n",
        "    network = network.to(device)\n",
        "\n",
        "    # initialize the kernel matrix\n",
        "    Ktemp = torch.tensor(np.zeros((batch_size,batch_size))).to(device)\n",
        "    data_iterator = iter(train_loader)\n",
        "    x, target = next(data_iterator)\n",
        "    x, target = x.to(device), target.to(device)\n",
        "    # forward data to network \n",
        "    network(x)\n",
        "    # log of the determinant\n",
        "    score = hooklogdet(Ktemp.cpu().detach().numpy(), target)\n",
        "    \n",
        "    del Ktemp, network, data_iterator\n",
        "\n",
        "    acc = api.get_more_info(int(i),dataset,is_random=False,hp=200)[\"test-accuracy\"]\n",
        "    \n",
        "    # considering time to compute the score\n",
        "    t = time.time()-start\n",
        "    csv_dict = {'Dataset': dataset, 'Network': i, 'Metric': score, 'Accuracy': acc, 'Time': t}\n",
        "    result = pd.DataFrame([csv_dict])\n",
        "    result.to_csv(f'out_{dataset}.csv', mode='a', index=False, header=False)\n",
        "    del result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 2"
      ],
      "metadata": {
        "id": "7h2LSeeAyM-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) NASWOT search algorithm for picking the higher naswot metric model using different sample sizes"
      ],
      "metadata": {
        "id": "jEZOg1WaqHfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 30\n",
        "sample_sizes = [50,100,200,400]\n",
        "\n",
        "for dataset in datasets:  \n",
        "  for sample in sample_sizes:\n",
        "    for run in range(iterations):\n",
        "      #get the train loader for each dataset with its own data augmentation and transformation\n",
        "      train_loader, batch_size = get_datasets(dataset)\n",
        "\n",
        "      scores = []\n",
        "      #pick a random sample of indexes in order to find the best one between them\n",
        "      networks = np.random.randint(0,len(api),size=sample)  \n",
        "      \n",
        "      start = time.time() \n",
        "\n",
        "      for i in networks:\n",
        "        #starting the configuration of each network\n",
        "        config = api.get_net_config(i, dataset)\n",
        "        network = get_cell_based_tiny_net(config)\n",
        "        network.apply(init)\n",
        "        #applying on each Relu a forward hook\n",
        "        for name, module in network.named_modules():\n",
        "          if (isinstance(module, torch.nn.modules.activation.ReLU)):\n",
        "            module.register_forward_hook(counting_forward_hook)\n",
        "\n",
        "        network = network.to(device)\n",
        "        # initialize the kernel matrix\n",
        "        Ktemp = torch.tensor(np.zeros((batch_size,batch_size))).to(device)\n",
        "        data_iterator = iter(train_loader)\n",
        "        x, target = next(data_iterator)\n",
        "        x, target = x.to(device), target.to(device)\n",
        "        # forward data to network \n",
        "        network(x)\n",
        "        score = hooklogdet(Ktemp.cpu().detach().numpy(), target)\n",
        "        \n",
        "        del Ktemp, network, data_iterator\n",
        "        \n",
        "        scores.append(score)\n",
        "\n",
        "      #select the best scoring function based on the naswot metric score\n",
        "      best_net = np.argmax(scores)\n",
        "      acc = api.get_more_info(int(networks[best_net]),dataset,is_random=False,hp=200)['test-accuracy']\n",
        "      t = time.time()-start\n",
        "      #save the name of the dataset, the number of sample, the index of the correspondant network, the naswot metric score, the accuracy for 200 epochs and finally the time to calculate everything\n",
        "      csv_dict = {'Dataset': dataset, 'Sample_size' : sample,'Network': networks[best_net], 'Metric': scores[best_net], 'Accuracy': acc, 'Time': t}\n",
        "      result = pd.DataFrame([csv_dict])\n",
        "      result.to_csv('scores.csv', mode='a', index=False, header=False )"
      ],
      "metadata": {
        "id": "5VkQ-47g54zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to calculate means and standard deviations for:\n",
        "### -metric\n",
        "### -accuracy for 200 epochs\n",
        "### -time"
      ],
      "metadata": {
        "id": "NoL5tcE4SsGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/scores.csv\",names = ['Dataset','size','Network','Metric','Accuracy','Time'], header = None, index_col=['Dataset','Network'])\n",
        "\n",
        "metric = df.groupby(by = ['Dataset','size'])['Metric'].describe()[['mean','std']]\n",
        "accuracy = df.groupby(by = ['Dataset','size'])['Accuracy'].describe()[['mean','std']]\n",
        "time = df.groupby(by = ['Dataset','size'])['Time'].describe()[['mean','std']]\n",
        "data = pd.concat([metric,accuracy,time],axis=1)\n",
        "data.to_csv('stats_naswot.csv')\n",
        "data.columns = ['metric_mean','metric_std','acc_mean','acc_std','time_mean','time_std']"
      ],
      "metadata": {
        "id": "-VnOZyKAALaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) NASWOT search using validation accuracy for picking optimal network after 12 epochs of training, considering different sizes"
      ],
      "metadata": {
        "id": "wdx_xwJYz6Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 30\n",
        "sample_sizes = [50,100,200,400]\n",
        "\n",
        "for dataset in datasets:  \n",
        "  for sample in sample_sizes:\n",
        "    for run in range(iterations):\n",
        "\n",
        "      #  training time\n",
        "      t = 0\n",
        "\n",
        "      start = time.time()\n",
        "\n",
        "      # save the 12 epoch accuracies\n",
        "      accuracies = []\n",
        "      networks = np.random.randint(0,len(api),size=sample)\n",
        "\n",
        "      for i in networks:\n",
        "        # get_more_info gives accuracy and time to train for 12 epoches\n",
        "        accuracies.append(api.get_more_info(int(i),dataset,is_random=False,hp=12)['test-accuracy']) \n",
        "        t += api.get_more_info(int(i),dataset,is_random=False,hp=12)['train-all-time']\n",
        "\n",
        "      # choose the best network for 12 epochs test-accuracy\n",
        "      best_net = np.argmax(accuracies)\n",
        "      \n",
        "      # consider the 200 epoch test accuracy \n",
        "      final_acc = api.get_more_info(int(networks[best_net]),dataset,is_random=False,hp=200)['test-accuracy']\n",
        "      t += (time.time()-start)\n",
        "      #save the name of the dataset, the number of sample, the index of the correspondant network, \n",
        "      #the accuracy for 12 epochs, the accuracy for 200 epochs and finally the time to:\n",
        "      ## -train for 12 epochs taken from the api\n",
        "      ## -calculate accuracies for 12 and 200 epochs\n",
        "      ## -choose the best network between accuracy for 12 epochs\n",
        "      csv_dict = {'Dataset': dataset, 'size':sample, 'Network': networks[best_net], 'Acc12Epochs': accuracies[best_net], 'Acc200Epochs': final_acc, 'Time': t}\n",
        "      result = pd.DataFrame([csv_dict])\n",
        "      result.to_csv('accuracies.csv', mode='a', index=False, header=False )"
      ],
      "metadata": {
        "id": "MFyG26imuF6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to calculate means and standard deviations for:\n",
        "### -accuracy for 12 epochs\n",
        "### -accuracy for 200 epochs\n",
        "### -time"
      ],
      "metadata": {
        "id": "EM5YOWXuWstP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/accuracies.csv\",names = ['Dataset','size','Network','Accuracy12','Accuracy200','Time'], header = None, index_col=['Dataset','Network'])\n",
        "accuracy12 = df.groupby(by = ['Dataset','size'])['Accuracy12'].describe()[['mean','std']]\n",
        "accuracy200 = df.groupby(by = ['Dataset','size'])['Accuracy200'].describe()[['mean','std']]\n",
        "time = df.groupby(by = ['Dataset','size'])['Time'].describe()[['mean','std']]\n",
        "data = pd.concat([accuracy12,accuracy200,time],axis=1)\n",
        "data.to_csv('stats_accuracy.csv')\n",
        "data.columns = ['acc12_mean','acc12_std','acc_mean','acc_std','time_mean','time_std']"
      ],
      "metadata": {
        "id": "yUMQlw4xAdVu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "W6hmAOmD5ooC"
      ],
      "name": "step1_and_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}