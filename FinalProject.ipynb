{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NAS+REA+SYNFLOW(solved).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WR58b8jo2434",
        "WKOjbEVOtU_3",
        "iI9hRkd-Bz4a",
        "gFCZ1Ti7e2n_",
        "86w_rZ7hIXle"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import of NATS-Bench and ImageNet16"
      ],
      "metadata": {
        "id": "4klm0NORjstM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nats_bench\n",
        "!pip install xautodl"
      ],
      "metadata": {
        "id": "N1FOaPW8OIWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSSTbRCs52BG"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "#from xautodl.datasets.DownsampledImageNet import ImageNet16\n",
        "from nats_bench import create\n",
        "!wget 'https://www.dropbox.com/s/pasubh1oghex3g9/?dl=1' -O 'NATS-tss-v1_0-3ffb9-simple.tar'\n",
        "#!wget 'https://www.dropbox.com/s/o2fg17ipz57nru1/?dl=1' -O ImageNet16.tar.gz\n",
        "!tar xvf \"NATS-tss-v1_0-3ffb9-simple.tar\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://www.dropbox.com/s/o2fg17ipz57nru1/?dl=1' -O ImageNet16.tar.gz\n",
        "file = tarfile.open('ImageNet16.tar.gz')\n",
        "file.extractall('.')\n",
        "file.close()"
      ],
      "metadata": {
        "id": "0hMby3GbZNmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports of packages"
      ],
      "metadata": {
        "id": "YOGw0mgwjv0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports \n",
        "import numpy as np, collections\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from xautodl.models import get_cell_based_tiny_net, get_search_spaces, CellStructure, get_search_spaces\n",
        "from xautodl.utils import get_model_infos, obtain_accuracy\n",
        "from xautodl.datasets.DownsampledImageNet import ImageNet16\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "from scipy import stats\n",
        "import time\n",
        "import collections\n",
        "import os, sys, time, glob, random, argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "# Create the API instance for the topology search space in NATS\n",
        "from nats_bench import create\n",
        "api = create(\"/content/NATS-tss-v1_0-3ffb9-simple\", 'tss', fast_mode=True, verbose=False)\n",
        "#assert torch.cuda.is_available(), \"CUDA is not available.\"\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "batch_size = 128\n"
      ],
      "metadata": {
        "id": "eN4x_stb6Lsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings for trainloader and data augmentation"
      ],
      "metadata": {
        "id": "SGQFMA0kjx4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## https://github.com/D-X-Y/AutoDL-Projects/blob/f46486e21b71ae6459a700be720d7648b5429569/xautodl/datasets/get_dataset_with_transform.py#L99\n",
        "def get_datasets(name):\n",
        "    if name == \"cifar10\":\n",
        "        mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
        "        std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
        "    elif name == \"cifar100\":\n",
        "        mean = [x / 255 for x in [129.3, 124.1, 112.4]]\n",
        "        std = [x / 255 for x in [68.2, 65.4, 70.4]]\n",
        "    elif name.startswith(\"ImageNet16\"):\n",
        "        mean = [x / 255 for x in [122.68, 116.66, 104.01]]\n",
        "        std = [x / 255 for x in [63.22, 61.26, 65.09]]\n",
        "    else:\n",
        "        raise TypeError(\"Unknown dataset : {:}\".format(name))\n",
        "\n",
        "    # Data Argumentation\n",
        "    if name == \"cifar10\" or name == \"cifar100\":\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ])\n",
        "    elif name.startswith(\"ImageNet16\"):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ])\n",
        "    \n",
        "    if name == \"cifar10\": \n",
        "      trainset = dset.CIFAR10(\"/content/Cifar10\", train=True, transform = transform, download=True)\n",
        "    elif name == \"cifar100\": \n",
        "      trainset = dset.CIFAR100(\"/content/Cifar100\", train=True ,transform = transform, download=True)\n",
        "    elif name.startswith(\"ImageNet16\"): \n",
        "      trainset = ImageNet16(\"ImageNet16\", train=True, transform = transform)\n",
        "    else:\n",
        "      raise TypeError(\"Unknown dataset : {:}\".format(name))\n",
        "  \n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=0, pin_memory = True)\n",
        "    return trainloader"
      ],
      "metadata": {
        "id": "czEvsnSEQQIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\"cifar10\", \"cifar100\", \"ImageNet16-120\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Metric', 'Accuracy', 'Time'])\n"
      ],
      "metadata": {
        "id": "IoZoV_GXQQAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SYNFLOW"
      ],
      "metadata": {
        "id": "WR58b8jo2434"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf zerocostnas/"
      ],
      "metadata": {
        "id": "IrBGyY9JbtcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SamsungLabs/zero-cost-nas.git\n",
        "import os\n",
        "os.rename(\"zero-cost-nas\", \"zerocostnas\")\n",
        "from zerocostnas.foresight.models import nasbench2, nasbench2_ops\n",
        "from zerocostnas.foresight.pruners import p_utils\n",
        "from zerocostnas.foresight.pruners.measures import *\n",
        "from zerocostnas.foresight.pruners import predictive\n",
        "###SYNFLOW from github\n",
        "#rename folder zero-cost-nas\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def synflow(arch_str, dataset, train_loader, device, dataload='random', dataload_info=1):\n",
        "  start = time.time()\n",
        "  def get_nclasses(dataset):\n",
        "    return 100 if dataset == 'cifar100' else 10 if dataset == 'cifar10' else 120\n",
        "\n",
        "  net = nasbench2.get_model_from_arch_str(arch_str.tostr(), get_nclasses(dataset))\n",
        "  measures = predictive.find_measures(net, train_loader, (dataload, dataload_info, get_nclasses(dataset)), device, measure_names=['synflow'])\n",
        "  return math.log1p(measures['synflow']), time.time() - start ###\n",
        "\n"
      ],
      "metadata": {
        "id": "A-LksPnCXn8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REA with NASWOT without modifications"
      ],
      "metadata": {
        "id": "iI9hRkd-Bz4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def counting_forward_hook(module, inp, out):\n",
        "  if isinstance(inp, tuple):\n",
        "      inp = inp[0]\n",
        "  inp = inp.view(inp.size(0), -1)\n",
        "  x = (inp > 0).float()\n",
        "  \n",
        "  K = x @ x.t()\n",
        "  K2 = (1.-x) @ (1.-x.t())\n",
        "  global Ktemp\n",
        "  Ktemp = Ktemp + K.cpu().numpy() + K2.cpu().numpy()\n",
        "\n",
        "def init(m):\n",
        "    if isinstance(m, (torch.nn.Conv2d, torch.nn.Linear)):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "# this is the logarithm of the determinant of K \n",
        "def hooklogdet(Ktemp, labels=None):\n",
        "  s, ld = np.linalg.slogdet(Ktemp)\n",
        "  return ld\n",
        "  \n",
        "def kernel(arch, dataset, batch_size, api, train_loader):\n",
        "    start = time.time()\n",
        "    index = api.query_index_by_arch(arch.tostr())\n",
        "    config = api.get_net_config(index, dataset)  \n",
        "    \n",
        "    network = get_cell_based_tiny_net(config)\n",
        "    del config\n",
        "    network.apply(init)\n",
        "\n",
        "    global Ktemp\n",
        "    Ktemp = np.zeros((batch_size,batch_size))\n",
        "    for name, module in network.named_modules():\n",
        "      if 'ReLU' in str(type(module)):\n",
        "        module.register_forward_hook(counting_forward_hook)\n",
        "    \n",
        "    network = network.to(device)\n",
        "    data_iterator = iter(train_loader)\n",
        "    x, target = next(data_iterator)\n",
        "    x, target = x.to(device), target.to(device)\n",
        "    network(x)\n",
        "    logdet= hooklogdet(Ktemp, target)\n",
        "    #print(\"logdet: \", logdet)\n",
        "    return logdet, time.time() - start\n",
        "\n",
        "from collections import defaultdict\n",
        "def random_topology_func(op_names, max_nodes=4):\n",
        "  # Return a random architecture\n",
        "  def random_architecture():\n",
        "      genotypes = []\n",
        "      for i in range(1, max_nodes):\n",
        "        xlist = []\n",
        "        for j in range(i):\n",
        "          node_str = \"{:}<-{:}\".format(i, j)\n",
        "          op_name = random.choice(op_names)\n",
        "          xlist.append((op_name, j))\n",
        "        genotypes.append(tuple(xlist))\n",
        "      \n",
        "      return CellStructure(genotypes)\n",
        "      \n",
        "  return random_architecture\n",
        "\n",
        "def mutate_topology_func(op_names):\n",
        "    \"\"\"Computes the architecture for a child of the given parent architecture.\n",
        "    The parent architecture is cloned and mutated to produce the child architecture. \n",
        "    The child architecture is mutated by randomly switch one operation to another.\n",
        "    \"\"\"\n",
        "    def mutate_architecture(parent_arch):\n",
        "        child_arch = deepcopy(parent_arch)\n",
        "        node_id = random.randint(0, len(child_arch.nodes) - 1)\n",
        "        node_info = list(child_arch.nodes[node_id])\n",
        "        snode_id = random.randint(0, len(node_info) - 1)\n",
        "        xop = random.choice(op_names)\n",
        "        while xop == node_info[snode_id][0]:\n",
        "            xop = random.choice(op_names)\n",
        "        node_info[snode_id] = (xop, node_info[snode_id][1])\n",
        "        child_arch.nodes[node_id] = tuple(node_info)\n",
        "        return child_arch\n",
        "\n",
        "    return mutate_architecture\n",
        "\n",
        "model = { \"arch\": None, \"synflow\": None}\n",
        "child = { \"arch\": None, \"synflow\": None}\n",
        "\n",
        "def regularized_evolution_kernel(\n",
        "    cycles,\n",
        "    population_size,\n",
        "    sample_size,\n",
        "    random_arch,\n",
        "    mutate_arch,\n",
        "    api,\n",
        "    dataset,\n",
        "    train_loader\n",
        "):\n",
        "    \"\"\"Algorithm for regularized evolution (i.e. aging evolution).\n",
        "    Follows \"Algorithm 1\" in Real et al. \"Regularized Evolution for Image\n",
        "    Classifier Architecture Search\".\n",
        "    Args:\n",
        "      cycles: the number of cycles the algorithm should run for.\n",
        "      population_size: the number of individuals to keep in the population.\n",
        "      sample_size: the number of individuals that should participate in each tournament.\n",
        "      time_budget: the upper bound of searching cost\n",
        "    Returns:\n",
        "      history: a list of `Model` instances, representing all the models computed\n",
        "          during the evolution experiment.\n",
        "    \"\"\"\n",
        "    population = []\n",
        "    history, total_time_cost = (\n",
        "        [],\n",
        "        [],\n",
        "    )  # Not used by the algorithm, only used to report results.\n",
        "    current_best_index = []\n",
        "    # Initialize the population with random models.\n",
        "    while len(population) < population_size:\n",
        "        model[\"arch\"] = random_arch()\n",
        "        model[\"metric\"], total_cost = kernel( ###### \n",
        "            model[\"arch\"], dataset, batch_size, api, train_loader #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population.append((model[\"metric\"], model[\"arch\"]))\n",
        "        history.append((model[\"metric\"], model[\"arch\"]))\n",
        "        #print(history)\n",
        "        # print(max(history, key=lambda x: x[0]))\n",
        "        total_time_cost.append(total_cost)\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1])\n",
        "        )\n",
        "    #print(total_time_cost)\n",
        "    #print(\"pop: \\n\", population)\n",
        "    \n",
        "    # Carry out evolution in cycles. Each cycle produces a model and removes another.\n",
        "    while len(history) < cycles:\n",
        "        # Sample randomly chosen models from the current population.\n",
        "        start_time, sample = time.time(), []\n",
        "        while len(sample) < sample_size:\n",
        "            # Inefficient, but written this way for clarity. In the case of neural\n",
        "            # nets, the efficiency of this line is irrelevant because training neural\n",
        "            # nets is the rate-determining step.\n",
        "            candidate = random.choice(list(population))\n",
        "            sample.append(candidate)\n",
        "        # The parent is the best model in the sample.\n",
        "        #print(\"sample:\\n\", sample)\n",
        "        parent = max(sample, key=lambda i: i[0])\n",
        "        #print(\"p:\\n\", parent[1], parent[0])\n",
        "        # Create the child model and store it.\n",
        "        child[\"arch\"] = mutate_arch(parent[1])\n",
        "        #print(\"c:\\n\", child['arch'])\n",
        "        child[\"metric\"], total_cost = kernel( ###### \n",
        "            child[\"arch\"], dataset, batch_size, api, train_loader #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        #print(\"c:\\n\", child['arch'], child['metric'])\n",
        "        # Append the info\n",
        "        population[:-1] = population[1:] ##popleft\n",
        "        population.append((child[\"metric\"], child[\"arch\"]))\n",
        "        #print('pop after:\\n', population)\n",
        "        history.append((child[\"metric\"], child[\"arch\"]))\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1])\n",
        "        )\n",
        "        total_time_cost.append(total_cost)\n",
        "        del sample, candidate, parent\n",
        "\n",
        "    return history, current_best_index, total_time_cost\n",
        "\n",
        "search_space = get_search_spaces(\"tss\", \"nats-bench\") \n",
        "random_arch = random_topology_func(op_names = search_space) \n",
        "mutate_arch = mutate_topology_func(search_space)"
      ],
      "metadata": {
        "id": "ppYCedASBfKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REA with Synflow without modifications\n"
      ],
      "metadata": {
        "id": "gFCZ1Ti7e2n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = { \"arch\": None, \"synflow\": None}\n",
        "child = { \"arch\": None, \"synflow\": None}\n",
        "\n",
        "def regularized_evolution_synflow(\n",
        "    cycles,\n",
        "    population_size,\n",
        "    sample_size,\n",
        "    random_arch,\n",
        "    mutate_arch,\n",
        "    api,\n",
        "    dataset,\n",
        "    train_loader,\n",
        "    device\n",
        "):\n",
        "\n",
        "    population = []\n",
        "    hist, total_time_cost = (\n",
        "        [],\n",
        "        [],\n",
        "    )  # Not used by the algorithm, only used to report results.\n",
        "    current_best_index = []\n",
        "    # Initialize the population with random models.\n",
        "    #print('inizio:\\n')\n",
        "    while len(population) < population_size:\n",
        "        model[\"arch\"] = random_arch()\n",
        "        #print(model)\n",
        "        model[\"synflow\"], total_cost = synflow( ###### \n",
        "            model[\"arch\"], dataset, train_loader, device, #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population.append(copy.deepcopy(model))\n",
        "        hist.append((model[\"synflow\"], model[\"arch\"]))\n",
        "        total_time_cost.append(total_cost)\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(hist, key=lambda x: x[0])[1]) ############\n",
        "        )\n",
        "    ## population has the same values repeated in it\n",
        "    #print(population)\n",
        "    #print(total_time_cost)\n",
        "    # Carry out evolution in cycles. Each cycle produces a model and removes another.\n",
        "    while len(hist) < cycles:\n",
        "        # Sample randomly chosen models from the current population.\n",
        "        start_time, sample = time.time(), []\n",
        "        while len(sample) < sample_size:\n",
        "            # Inefficient, but written this way for clarity. In the case of neural\n",
        "            # nets, the efficiency of this line is irrelevant because training neural\n",
        "            # nets is the rate-determining step.\n",
        "            candidate = random.choice(list(population))\n",
        "            #print(candidate)\n",
        "            sample.append(candidate) #sample is a list of dicts\n",
        "\n",
        "        # The parent is the best model in the sample.\n",
        "        #print(sample) ##always the same cause of population\n",
        "        parent = max(sample, key=lambda i: i[\"synflow\"]) ## adjust the sample to take the maximum\n",
        "        #print(type(parent))\n",
        "        # Create the child model and store it.\n",
        "        child[\"arch\"] = mutate_arch(parent[\"arch\"])\n",
        "        child[\"synflow\"], total_cost = synflow( ###### \n",
        "            child[\"arch\"], dataset, train_loader, device, #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population[:-1] = population[1:] ##popleft\n",
        "        population[-1] = child\n",
        "        hist.append((child[\"synflow\"], child[\"arch\"]))\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(hist, key=lambda x: x[0])[1]) ############\n",
        "        )\n",
        "        total_time_cost.append(total_cost)\n",
        "\n",
        "    return hist, current_best_index, total_time_cost\n",
        "\n",
        "search_space = get_search_spaces(\"tss\", \"nats-bench\") \n",
        "random_arch = random_topology_func(op_names = search_space) \n",
        "mutate_arch = mutate_topology_func(search_space)\n"
      ],
      "metadata": {
        "id": "cxn0mV8hOviS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REA with NASWOT with condition on SKIP_CONNECTION\n",
        "Considering a minibatch of data $X = \\{  {x_{i} \\} }_{i=1}^{N}$ and a network, we can define an indicator variable $c_{i}$ that form a binary code that corresponds to a linear region. We can use the hamming distance between two data points $d_{H}(c_{i},c_{j})$ to measure how dissimilar the two points are. The hamming distance is the minimum number of changes needed to turn one binary code in the other. Or we could also say it is the number of positions where the two codes differ. The correspondance between binary codes can then be calculated by $K_{H}$ with $N_{A}$ as the number of ReLu activations in the network.\n",
        "\n",
        "$$\n",
        "K_{H} = \\begin{bmatrix} \n",
        "    N_{A} - d_{H}(c_{1},c_{1}) & \\dots & N_{A} - d_{H}(c_{1},c_{N}) \\\\\n",
        "    \\vdots & \\ddots & \\vdots \\\\\n",
        "    N_{A} - d_{H}(c_{N},c_{1}) &  \\dots      & N_{A} - d_{H}(c_{N},c_{N}) \n",
        "    \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The scoring metric $s$ is therefore derived from the logarithm of the Kernel norm at initialization. \n",
        "\n",
        "$$\n",
        "s = \\log \\lVert K_{H} \\rVert\n",
        "$$\n"
      ],
      "metadata": {
        "id": "zJ-BRX9OtG1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_naswotmod_func(op_names, max_nodes=4):\n",
        "  # Return a random architecture\n",
        "  def random_m_architecture():\n",
        "      genotypes = []\n",
        "      found = False\n",
        "      while found == False:\n",
        "        for i in range(1, max_nodes):\n",
        "            xlist = []\n",
        "            for j in range(i):\n",
        "                op_name = random.choice(op_names)\n",
        "                xlist.append((op_name, j))\n",
        "            genotypes.append(tuple(xlist))\n",
        "        #print(CellStructure(genotypes).tostr())\n",
        "        if (CellStructure(genotypes).tostr().count('skip')>=0 & CellStructure(genotypes).tostr().count('skip')<=1): \n",
        "          #print('skip')\n",
        "          found=True\n",
        "        else:\n",
        "          genotypes = []\n",
        "      \n",
        "      return CellStructure(genotypes)\n",
        "      \n",
        "  return random_m_architecture\n",
        "\n",
        "def mutate_naswotmod_func(op_names):\n",
        "    \"\"\"Computes the architecture for a child of the given parent architecture.\n",
        "    The parent architecture is cloned and mutated to produce the child architecture. \n",
        "    The child architecture is mutated by randomly switch one operation to another.\n",
        "    \"\"\"\n",
        "    def mutate_m_architecture(parent_arch):\n",
        "      found= False\n",
        "      while found==False:\n",
        "          child_arch = deepcopy(parent_arch)\n",
        "          node_id = random.randint(0, len(child_arch.nodes) - 1)\n",
        "          node_info = list(child_arch.nodes[node_id])\n",
        "          snode_id = random.randint(0, len(node_info) - 1)\n",
        "          xop = random.choice(op_names)\n",
        "          while xop == node_info[snode_id][0]:\n",
        "              xop = random.choice(op_names)\n",
        "          node_info[snode_id] = (xop, node_info[snode_id][1])\n",
        "          child_arch.nodes[node_id] = tuple(node_info)\n",
        "          if child_arch.tostr().count('skip')==1: \n",
        "            found=True\n",
        "            return child_arch\n",
        "        \n",
        "    return mutate_m_architecture\n",
        "\n",
        "\n",
        "model = { \"arch\": None, \"metric\": None}\n",
        "child = { \"arch\": None, \"metric\": None}\n",
        "\n",
        "\n",
        "## P/S of: 100/2, 100/50, 20/20, 100/25, 54/16 ; best is 100/25\n",
        "def regularized_evolution_kernel_m(\n",
        "    cycles,\n",
        "    population_size,\n",
        "    sample_size,\n",
        "    random_arch,\n",
        "    mutate_arch,\n",
        "    api,\n",
        "    dataset,\n",
        "    train_loader\n",
        "):\n",
        "\n",
        "    population = []\n",
        "    history, total_time_cost = (\n",
        "        [],\n",
        "        [],\n",
        "    )  # Not used by the algorithm, only used to report results.\n",
        "    current_best_index = []\n",
        "    # Initialize the population with random models.\n",
        "    while len(population) < population_size:\n",
        "        model[\"arch\"] = random_naswotmod()\n",
        "        model[\"metric\"], total_cost = kernel( ###### \n",
        "            model[\"arch\"], dataset, batch_size, api, train_loader #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population.append((model[\"metric\"], model[\"arch\"]))\n",
        "        history.append((model[\"metric\"], model[\"arch\"]))\n",
        "\n",
        "        total_time_cost.append(total_cost)\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1])\n",
        "        )\n",
        "\n",
        "    \n",
        "    # Carry out evolution in cycles. Each cycle produces a model and removes another.\n",
        "    while len(history) < cycles:\n",
        "        # Sample randomly chosen models from the current population.\n",
        "        start_time, sample = time.time(), []\n",
        "        while len(sample) < sample_size:\n",
        "            # Inefficient, but written this way for clarity. In the case of neural\n",
        "            # nets, the efficiency of this line is irrelevant because training neural\n",
        "            # nets is the rate-determining step.\n",
        "            candidate = random.choice(list(population))\n",
        "            sample.append(candidate)\n",
        "        # The parent is the best model in the sample.\n",
        "        #print(\"sample:\\n\", sample)\n",
        "        parent = max(sample, key=lambda i: i[0])\n",
        "        #print(\"p:\\n\", parent[1], parent[0])\n",
        "        # Create the child model and store it.\n",
        "        child[\"arch\"] = mutate_naswotmod(parent[1])\n",
        "        #print(\"c:\\n\", child['arch'])\n",
        "        child[\"metric\"], total_cost = kernel( ###### \n",
        "            child[\"arch\"], dataset, batch_size, api, train_loader #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        #print(\"c:\\n\", child['arch'], child['metric'])\n",
        "        # Append the info\n",
        "        population[:-1] = population[1:] ##popleft\n",
        "        population.append((child[\"metric\"], child[\"arch\"]))\n",
        "        #print('pop after:\\n', population)\n",
        "        history.append((child[\"metric\"], child[\"arch\"]))\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1])\n",
        "        )\n",
        "        total_time_cost.append(total_cost)\n",
        "        del sample, candidate, parent\n",
        "\n",
        "    return history, current_best_index, total_time_cost\n",
        "\n",
        "search_space = get_search_spaces(\"tss\", \"nats-bench\") \n",
        "random_naswotmod = random_naswotmod_func(op_names = search_space) \n",
        "mutate_naswotmod = mutate_naswotmod_func(search_space)"
      ],
      "metadata": {
        "id": "YE1VmNBcDypy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REA with Synflow with condition on SKIP_CONNECTION"
      ],
      "metadata": {
        "id": "86w_rZ7hIXle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_synflowmod_func(op_names, max_nodes=4):\n",
        "  # Return a random architecture\n",
        "  def random_m_architecture():\n",
        "      genotypes = []\n",
        "      found = False\n",
        "      while found == False:\n",
        "        for i in range(1, max_nodes):\n",
        "            xlist = []\n",
        "            for j in range(i):\n",
        "                op_name = random.choice(op_names)\n",
        "                xlist.append((op_name, j))\n",
        "            genotypes.append(tuple(xlist))\n",
        "\n",
        "        if ((CellStructure(genotypes).tostr().count('skip')==1) | (CellStructure(genotypes).tostr().count('skip')==0)): #### controllo skip in MODIFICAAAAAAAAA\n",
        "          print('skip')\n",
        "          found=True\n",
        "        else:\n",
        "          genotypes = []\n",
        "      \n",
        "      return CellStructure(genotypes)\n",
        "      \n",
        "  return random_m_architecture\n",
        "\n",
        "def mutate_synflowmod_func(op_names):\n",
        "    \"\"\"Computes the architecture for a child of the given parent architecture.\n",
        "    The parent architecture is cloned and mutated to produce the child architecture. \n",
        "    The child architecture is mutated by randomly switch one operation to another.\n",
        "    \"\"\"\n",
        "    def mutate_m_architecture(parent_arch):\n",
        "        child_arch = deepcopy(parent_arch)\n",
        "        node_id = random.randint(0, len(child_arch.nodes) - 1)\n",
        "        node_info = list(child_arch.nodes[node_id])\n",
        "        snode_id = random.randint(0, len(node_info) - 1)\n",
        "        xop = random.choice(op_names)\n",
        "        while xop == node_info[snode_id][0]:\n",
        "            xop = random.choice(op_names)\n",
        "        node_info[snode_id] = (xop, node_info[snode_id][1])\n",
        "        child_arch.nodes[node_id] = tuple(node_info)\n",
        "        return child_arch\n",
        "\n",
        "    return mutate_m_architecture\n",
        "\n",
        "model = { \"arch\": None, \"synflow\": None}\n",
        "child = { \"arch\": None, \"synflow\": None}\n",
        "\n",
        "\n",
        "## P/S of: 100/2, 100/50, 20/20, 100/25, 54/16 ; best is 100/25\n",
        "def regularized_evolution_synflow_m(\n",
        "    cycles,\n",
        "    population_size,\n",
        "    sample_size,\n",
        "    random_arch,\n",
        "    mutate_arch,\n",
        "    api,\n",
        "    dataset,\n",
        "    train_loader,\n",
        "    device\n",
        "):\n",
        "\n",
        "    population = []\n",
        "    history, total_time_cost = (\n",
        "        [],\n",
        "        [],\n",
        "    )  # Not used by the algorithm, only used to report results.\n",
        "    current_best_index = []\n",
        "    # Initialize the population with random models.\n",
        "    while len(population) < population_size:\n",
        "        model[\"arch\"] = random_synflowmod()\n",
        "        model[\"synflow\"], total_cost = synflow( ###### \n",
        "            model[\"arch\"], dataset, train_loader, device, #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population.append(copy.deepcopy(model))\n",
        "        history.append((model[\"synflow\"], model[\"arch\"]))\n",
        "        total_time_cost.append(total_cost)\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1]) ############\n",
        "        )\n",
        "    ## population has the same values repeated in it\n",
        "    #print(population)\n",
        "    #print(total_time_cost)\n",
        "    # Carry out evolution in cycles. Each cycle produces a model and removes another.\n",
        "    while len(history) < cycles:\n",
        "        # Sample randomly chosen models from the current population.\n",
        "        start_time, sample = time.time(), []\n",
        "        while len(sample) < sample_size:\n",
        "            # Inefficient, but written this way for clarity. In the case of neural\n",
        "            # nets, the efficiency of this line is irrelevant because training neural\n",
        "            # nets is the rate-determining step.\n",
        "            candidate = random.choice(list(population))\n",
        "            #print(candidate)\n",
        "            sample.append(candidate) #sample is a list of dicts\n",
        "\n",
        "        # The parent is the best model in the sample.\n",
        "        #print(sample) ##always the same cause of population\n",
        "        parent = max(sample, key=lambda i: i[\"synflow\"]) ## adjust the sample to take the maximum\n",
        "        #print(type(parent))\n",
        "        # Create the child model and store it.\n",
        "        child[\"arch\"] = mutate_synflowmod(parent[\"arch\"])\n",
        "        child[\"synflow\"], total_cost = synflow( ###### \n",
        "            child[\"arch\"], dataset, train_loader, device, #if use_proxy else api.full_train_epochs\n",
        "        )\n",
        "        # Append the info\n",
        "        population[:-1] = population[1:] ##popleft\n",
        "        population[-1] = child\n",
        "        history.append((child[\"synflow\"], child[\"arch\"]))\n",
        "        current_best_index.append(\n",
        "            api.query_index_by_arch(max(history, key=lambda x: x[0])[1]) ############\n",
        "        )\n",
        "        total_time_cost.append(total_cost)\n",
        "        del sample, candidate, parent\n",
        "\n",
        "    return history, current_best_index, total_time_cost\n",
        "\n",
        "search_space = get_search_spaces(\"tss\", \"nats-bench\") \n",
        "random_synflowmod = random_synflowmod_func(op_names = search_space) \n",
        "mutate_synflowmod = mutate_synflowmod_func(search_space)"
      ],
      "metadata": {
        "id": "OgPFSdACIU2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run with REA + NASWOT "
      ],
      "metadata": {
        "id": "M6A3EvWRTx6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ktemp = []\n",
        "for dataset in datasets:\n",
        "  for run in range(10):\n",
        "    train_loader = get_datasets(dataset)\n",
        "    ## REA \n",
        "    history, current_best_index, total_times = regularized_evolution_kernel(cycles = 200,\n",
        "                                                                    population_size = 50, \n",
        "                                                                    sample_size = 10, \n",
        "                                                                    random_arch= random_arch, \n",
        "                                                                    mutate_arch=mutate_arch,  \n",
        "                                                                    api=api, \n",
        "                                                                    dataset= dataset,\n",
        "                                                                    train_loader = train_loader,\n",
        "                                                                    )\n",
        "    best_arch = max(history, key=lambda x: x[0])[1]\n",
        "    best_score = max(history, key=lambda x: x[0])[0]\n",
        "    \n",
        "\n",
        "    index = api.query_index_by_arch(best_arch.tostr()) \n",
        "\n",
        "    csv_dict = {'Dataset': dataset, 'Network_index': index, 'Metric': best_score, 'Accuracy': api.get_more_info(int(index), dataset, hp=\"200\", is_random=False)[\"test-accuracy\"] , 'Time': sum(total_times)}\n",
        "\n",
        "    df_dict = pd.DataFrame([csv_dict])\n",
        "    result = pd.concat([result, df_dict], ignore_index=True)\n",
        "\n",
        "    result.to_csv(f'outKERNELwithoutMOD10runs_{dataset}.csv', mode='a', index=False, header=False )\n",
        "    result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Metric', 'Accuracy', 'Time'])"
      ],
      "metadata": {
        "id": "yz2C4o0YT0Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run with REA + NASWOT with skip filter"
      ],
      "metadata": {
        "id": "mcrYtYLeh4gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ktemp = []\n",
        "for dataset in datasets:\n",
        "  for run in range(10):\n",
        "    train_loader = get_datasets(dataset)\n",
        "    ## REA \n",
        "    history, current_best_index, total_times = regularized_evolution_kernel_m(cycles = 200,\n",
        "                                                                    population_size = 50, \n",
        "                                                                    sample_size = 10, \n",
        "                                                                    random_arch= random_naswotmod, \n",
        "                                                                    mutate_arch=mutate_naswotmod,  \n",
        "                                                                    api=api, \n",
        "                                                                    dataset= dataset,\n",
        "                                                                    train_loader = train_loader,\n",
        "                                                                    )\n",
        "    best_arch = max(history, key=lambda x: x[0])[1]\n",
        "    best_score = max(history, key=lambda x: x[0])[0]\n",
        "    \n",
        "\n",
        "    index = api.query_index_by_arch(best_arch.tostr()) \n",
        "\n",
        "    csv_dict = {'Dataset': dataset, 'Network_index': index, 'Metric': best_score, 'Accuracy': api.get_more_info(int(index), dataset, hp=\"200\", is_random=False)[\"test-accuracy\"] , 'Time': sum(total_times)}\n",
        "\n",
        "    df_dict = pd.DataFrame([csv_dict])\n",
        "    result = pd.concat([result, df_dict], ignore_index=True)\n",
        "\n",
        "    result.to_csv(f'outKERNELwithMOD10runs_{dataset}.csv', mode='a', index=False, header=False )\n",
        "    result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Metric', 'Accuracy', 'Time'])"
      ],
      "metadata": {
        "id": "s4y6fFr9h4gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run with REA + Synflow "
      ],
      "metadata": {
        "id": "fHbl_YdWiMJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ktemp = []\n",
        "for dataset in datasets:\n",
        "  for run in range(10):\n",
        "    train_loader = get_datasets(dataset)\n",
        "    ## REA \n",
        "    history, current_best_index, total_times = regularized_evolution_synflow(cycles = 200,\n",
        "                                                                    population_size = 50, \n",
        "                                                                    sample_size = 10, \n",
        "                                                                    random_arch= random_arch, \n",
        "                                                                    mutate_arch=mutate_arch,  \n",
        "                                                                    api=api, \n",
        "                                                                    dataset= dataset,\n",
        "                                                                    train_loader = train_loader,\n",
        "                                                                    device=device)\n",
        "    best_arch = max(history, key=lambda x: x[0])[1]\n",
        "    best_score = max(history, key=lambda x: x[0])[0]\n",
        "    \n",
        "\n",
        "    index = api.query_index_by_arch(best_arch.tostr()) \n",
        "\n",
        "    csv_dict = {'Dataset': dataset, 'Network_index': index, 'Synflow': best_score, 'Accuracy': api.get_more_info(int(index), dataset, hp=\"200\", is_random=False)[\"test-accuracy\"] , 'Time': sum(total_times)}\n",
        "\n",
        "    df_dict = pd.DataFrame([csv_dict])\n",
        "    result = pd.concat([result, df_dict], ignore_index=True)\n",
        "\n",
        "    result.to_csv(f'outSYNFLOWwithoutMOD10runs_{dataset}.csv', mode='a', index=False, header=False )\n",
        "    result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Synflow', 'Accuracy', 'Time'])"
      ],
      "metadata": {
        "id": "D0liub9miMJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run with REA + Synflow with skip filter\n"
      ],
      "metadata": {
        "id": "SZGncJl_iMJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ktemp = []\n",
        "for dataset in datasets:\n",
        "  for run in range(10):\n",
        "    train_loader = get_datasets(dataset)\n",
        "    ## REA \n",
        "    history, current_best_index, total_times = regularized_evolution_synflow_m(cycles = 200,\n",
        "                                                                    population_size = 50, \n",
        "                                                                    sample_size = 10, \n",
        "                                                                    random_arch= random_synflowmod, \n",
        "                                                                    mutate_arch=mutate_synflowmod,  \n",
        "                                                                    api=api, \n",
        "                                                                    dataset= dataset,\n",
        "                                                                    train_loader = train_loader,\n",
        "                                                                    device=device)\n",
        "    best_arch = max(history, key=lambda x: x[0])[1]\n",
        "    best_score = max(history, key=lambda x: x[0])[0]\n",
        "    \n",
        "\n",
        "    index = api.query_index_by_arch(best_arch.tostr()) \n",
        "\n",
        "    csv_dict = {'Dataset': dataset, 'Network_index': index, 'Synflow': best_score, 'Accuracy': api.get_more_info(int(index), dataset, hp=\"200\", is_random=False)[\"test-accuracy\"] , 'Time': sum(total_times)}\n",
        "\n",
        "    df_dict = pd.DataFrame([csv_dict])\n",
        "    result = pd.concat([result, df_dict], ignore_index=True)\n",
        "\n",
        "    result.to_csv(f'outSYNFLOWwithMOD10runs_{dataset}.csv', mode='a', index=False, header=False )\n",
        "    result = pd.DataFrame(columns=['Dataset', 'Network_index', 'Synflow', 'Accuracy', 'Time'])"
      ],
      "metadata": {
        "id": "CYPNzlQGiMJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLOTS for Cifar10"
      ],
      "metadata": {
        "id": "nJ-_wteRYg4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "!git clone https://github.com/jjguglielmi/Neural-Architecture-Search-without-Training.git\n",
        "\n",
        "\n",
        "def func(string):\n",
        "  count = ''\n",
        "  for i in range(0,3):\n",
        "    if(i!=0): count = count + ' '\n",
        "    tot = string.split('+')[i].count('skip')\n",
        "    count = count + str(tot)\n",
        "  return count\n",
        "\n",
        "# if you are running on Google Colab, left the variable 'csvs' like that; instead if you are not, remove '/content/'\n",
        "csvs = ['/content/Neural-Architecture-Search-without-Training/pre-trained-architectures/out_c10_200.csv', \n",
        "        '/content/Neural-Architecture-Search-without-Training/pre-trained-architectures/out_c100_200.csv', \n",
        "        '/content/Neural-Architecture-Search-without-Training/pre-trained-architectures/out_IN_200.csv']\n",
        "\n",
        "header=['Dataset', 'sample', 'index', 'LogDet', 'TestAccuracy', 'Time']\n",
        "score = '/content/Neural-Architecture-Search-without-Training/pre-trained-architectures/scores.csv'"
      ],
      "metadata": {
        "id": "W_axnjuiDs-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import reader\n",
        "import csv\n",
        "headerS=['Dataset', 'sample', 'index', 'LogDet', 'TestAccuracy', 'Time']\n",
        "headerSkip = ['Dataset', 'Arch', 'count_skip', 'pos_skip', 'TestAccuracy', 'index']\n",
        "\n",
        "with open(score, 'r') as read_obj:\n",
        "  csv_reader = reader(read_obj)\n",
        "  with open('skips.csv', 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    # write the header\n",
        "    writer.writerow(headerSkip)\n",
        "    for row in csv_reader:\n",
        "      for ds in datasets:\n",
        "        if row[0] == ds:\n",
        "          arch_str= api.get_net_config(int(row[2]), row[0])['arch_str']\n",
        "          l= [row[0],arch_str, int(arch_str.count('skip')), func(arch_str), row[4], row[2]]\n",
        "          writer.writerow(l)\n",
        "\n",
        "df= pd.read_csv('/content/skips.csv', names=headerSkip, skiprows=1)\n",
        "sns.set_style(\"white\")\n",
        "df=df.astype({'count_skip': int, 'TestAccuracy': float, 'Arch': str, 'pos_skip':str})\n",
        "x0 = df.loc[(df['count_skip']==0) & (df['Dataset']=='cifar10'),  ['TestAccuracy']]\n",
        "xS= df.loc[(df['count_skip']>0) & (df['Dataset']=='cifar10'), ['TestAccuracy']]\n",
        "x0['Skip/notSkip'] = 'notSkip'\n",
        "xS['Skip/notSkip'] = 'Skip'\n",
        "xTot= pd.concat([x0, xS])\n",
        "#print(df)\n",
        "plt.figure(figsize=(8,5), dpi= 80)\n",
        "sns.histplot(xTot, x='TestAccuracy', hue='Skip/notSkip', element='bars')\n",
        "\n",
        "plt.savefig(f'histSkip_noSkipScorescifar10.png')\n",
        "\n",
        "skip_df = df\n",
        "skip_count = skip_df['Arch'].str.count('skip')\n",
        "skip_df['count_skip'] = skip_count\n",
        "acc_count= skip_df[['TestAccuracy', 'count_skip', 'Dataset']].copy()\n",
        "plt.figure(figsize=(8,5), dpi= 80)\n",
        "sns.histplot(acc_count, x='TestAccuracy', hue='count_skip', element='poly')\n",
        "plt.title(f'Skip distribution for {ds}')\n",
        "plt.savefig(f'histSkipcifar10.png')\n",
        "df_skips= acc_count.loc[(acc_count['count_skip']<=3) & (acc_count['TestAccuracy']>20) & (acc_count['Dataset']=='cifar10'), ['TestAccuracy', 'count_skip']]\n",
        "plt.figure(figsize=(8,5), dpi= 80)\n",
        "plt.title(f'Distribution of architectures based on the number of skips ({ds})')\n",
        "sns.histplot(df_skips, x='TestAccuracy', hue='count_skip', element='poly', palette = 'copper_r')\n",
        "plt.savefig(f'histHighAccCifar10.png')\n",
        "df_skips.groupby(\"count_skip\").agg([\"mean\", \"std\"]).to_csv(f'comparisonMeanSTDSkipscifar10.csv') ## table mean/std of arch with 1/2/3 skips without outliers (acc>20)\n",
        "tot= skip_df.loc[(skip_df['count_skip']<=3) & (acc_count['TestAccuracy']>20) & (acc_count['Dataset']=='cifar10'), ['Arch', 'TestAccuracy', 'count_skip']]\n",
        "totPosSkip = tot.copy()\n",
        "totPosSkip['pos_countSkip'] = totPosSkip['Arch'].apply(lambda x: str(func(x)))\n",
        "new = totPosSkip.filter(['TestAccuracy', 'count_skip', 'pos_countSkip'])\n",
        "sorted = new.sort_values('TestAccuracy', ascending=False)\n",
        "##sorted.head(5).to_latex()\n",
        "sorted.head(10).to_csv(f'first10SortedArchwithSkipPoscifar10.csv')"
      ],
      "metadata": {
        "id": "McY7tVEWI4L0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}